{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_LDAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr",
        "outputId": "31205c89-98c4-47c2-83cf-1e70b3dbef90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6SYRFm9mr-"
      },
      "source": [
        "### Carregamento arquivo de pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY7uDiB9AG57",
        "outputId": "5c432c36-88b9-4b94-fab0-4ff1bffc4126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "%run \"/content/drive/My Drive/Colab Notebooks/pre_processamento_TCC.ipynb\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "CPU times: user 2min 7s, sys: 12.6 s, total: 2min 20s\n",
            "Wall time: 2min 25s\n",
            "Ataque de exploração UDPLag:  Label\n",
            "BENIGN       3705\n",
            "UDP-lag    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "BENIGN        392\n",
            "Syn       1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "BENIGN           1612\n",
            "DrDoS_LDAP    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "BENIGN              1707\n",
            "DrDoS_NetBIOS    4093279\n",
            "dtype: int64\n",
            "Ataque de exploração UDPLag:  Label\n",
            "0      3705\n",
            "1    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "0        392\n",
            "1    1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "0       1612\n",
            "1    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "0       1707\n",
            "1    4093279\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUE0XHHr3LzV",
        "outputId": "a3ce79fc-bb84-4a4c-ca10-9f83332b2a33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "LDAP = LDAP.sample(333763)\n",
        "print(LDAP.groupby(by=' Label').size())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Label\n",
            "0       245\n",
            "1    333518\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXdSSkdaH4WQ"
      },
      "source": [
        "### Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyNbLHJIBlJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWBHgHzHU9w"
      },
      "source": [
        "### Divisão do conjunto em treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53UJNEuJCQf",
        "outputId": "b8fa6d0b-6585-44eb-c37c-d8173a3b5302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "LDAP"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Fwd Avg Bytes/Bulk</th>\n",
              "      <th>Fwd Avg Packets/Bulk</th>\n",
              "      <th>Fwd Avg Bulk Rate</th>\n",
              "      <th>Bwd Avg Bytes/Bulk</th>\n",
              "      <th>Bwd Avg Packets/Bulk</th>\n",
              "      <th>Bwd Avg Bulk Rate</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1114649</th>\n",
              "      <td>48</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.133333e+07</td>\n",
              "      <td>4.166667e+04</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>4.166667e+04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109159</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.944000e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738046</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.472000e+09</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351193</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.944000e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2125437950</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2125437950</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1062718975</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606462</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.944000e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1958632</th>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.690909e+07</td>\n",
              "      <td>4.545455e+04</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4.545455e+04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1570733</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.944000e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1739505</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.472000e+09</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678195</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2944.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.944000e+09</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2208.0</td>\n",
              "      <td>1472.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2944</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1092406</th>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2928.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.856000e+07</td>\n",
              "      <td>4.000000e+04</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2896</td>\n",
              "      <td>0</td>\n",
              "      <td>4.000000e+04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2196.0</td>\n",
              "      <td>1464.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2896</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2928</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>1448</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333763 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Flow Duration   Total Fwd Packets  ...   Idle Min   Label\n",
              "1114649              48                   2  ...        0.0       1\n",
              "1109159               1                   2  ...        0.0       1\n",
              "738046                2                   2  ...        0.0       1\n",
              "1351193               1                   2  ...        0.0       1\n",
              "606462                1                   2  ...        0.0       1\n",
              "...                 ...                 ...  ...        ...     ...\n",
              "1958632              44                   2  ...        0.0       1\n",
              "1570733               1                   2  ...        0.0       1\n",
              "1739505               2                   2  ...        0.0       1\n",
              "678195                1                   2  ...        0.0       1\n",
              "1092406              50                   2  ...        0.0       1\n",
              "\n",
              "[333763 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ldV1PAMGk6h"
      },
      "source": [
        "Preparação dos dados treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JQDSHBGMm1"
      },
      "source": [
        "X = LDAP.iloc[:, 0:77]\n",
        "y = LDAP.iloc[:,- 1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1hcfSsGQEj"
      },
      "source": [
        "70% para treino, 30% para teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmiwVL7bDRaL"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWSoJT6XFgyZ",
        "outputId": "8cce46f2-b620-45fa-db38-fc7f9bef0f3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Formato dos dados de entrada\n",
        "print('Formato dos dados de entrada:', x_train.shape)\n",
        "\n",
        "# Tamanho dos conjuntos\n",
        "print('Amostras de treino: ', x_train.shape[0])\n",
        "print('Amostras de teste: ', x_test.shape[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Formato dos dados de entrada: (233634, 77)\n",
            "Amostras de treino:  233634\n",
            "Amostras de teste:  100129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGh7k5c6Hd2e"
      },
      "source": [
        "### Seleção dos Parâmetro\n",
        "Seleção dos 15 melhores parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UNx2xDtDg6K",
        "outputId": "f2964c3b-8af1-45c2-a126-439c66b80b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "best_features = SelectKBest(score_func=f_classif, k=15)\n",
        "fit = best_features.fit(x_train,y_train)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(x_train.columns)\n",
        "# concatenar quadros de dados\n",
        "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "feature_scores.columns = ['Feature_Name','Score']  # colunas de saída de nome\n",
        "print(feature_scores.nlargest(15,'Score'))  # imprima 15 melhores parâmetros"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Feature_Name          Score\n",
            "50            Down/Up Ratio  157487.581834\n",
            "47           URG Flag Count  154300.574231\n",
            "37        Min Packet Length  128140.330758\n",
            "6     Fwd Packet Length Min  128100.037219\n",
            "7    Fwd Packet Length Mean  126697.022156\n",
            "52     Avg Fwd Segment Size  126697.022156\n",
            "39       Packet Length Mean  122917.784177\n",
            "5     Fwd Packet Length Max  121881.836410\n",
            "51      Average Packet Size  118508.959261\n",
            "38        Max Packet Length   90507.311871\n",
            "10    Bwd Packet Length Min   74889.739572\n",
            "2    Total Backward Packets   38635.386150\n",
            "63      Subflow Bwd Packets   38635.386150\n",
            "65   Init_Win_bytes_forward   35616.860515\n",
            "28              Bwd IAT Min   30931.391337\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [30 31 32 42 45 49 55 56 57 58 59 60 70 74] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TRpDjFn-OC",
        "outputId": "9f7b2dad-a7f3-4407-b7f8-df9e9d33dca3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "feature = feature_scores.nlargest(15,'Score')\n",
        "feature"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature_Name</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Down/Up Ratio</td>\n",
              "      <td>157487.581834</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>URG Flag Count</td>\n",
              "      <td>154300.574231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Min Packet Length</td>\n",
              "      <td>128140.330758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Fwd Packet Length Min</td>\n",
              "      <td>128100.037219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Fwd Packet Length Mean</td>\n",
              "      <td>126697.022156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Avg Fwd Segment Size</td>\n",
              "      <td>126697.022156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Packet Length Mean</td>\n",
              "      <td>122917.784177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Fwd Packet Length Max</td>\n",
              "      <td>121881.836410</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Average Packet Size</td>\n",
              "      <td>118508.959261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Max Packet Length</td>\n",
              "      <td>90507.311871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Bwd Packet Length Min</td>\n",
              "      <td>74889.739572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Total Backward Packets</td>\n",
              "      <td>38635.386150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>Subflow Bwd Packets</td>\n",
              "      <td>38635.386150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Init_Win_bytes_forward</td>\n",
              "      <td>35616.860515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Bwd IAT Min</td>\n",
              "      <td>30931.391337</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Feature_Name          Score\n",
              "50            Down/Up Ratio  157487.581834\n",
              "47           URG Flag Count  154300.574231\n",
              "37        Min Packet Length  128140.330758\n",
              "6     Fwd Packet Length Min  128100.037219\n",
              "7    Fwd Packet Length Mean  126697.022156\n",
              "52     Avg Fwd Segment Size  126697.022156\n",
              "39       Packet Length Mean  122917.784177\n",
              "5     Fwd Packet Length Max  121881.836410\n",
              "51      Average Packet Size  118508.959261\n",
              "38        Max Packet Length   90507.311871\n",
              "10    Bwd Packet Length Min   74889.739572\n",
              "2    Total Backward Packets   38635.386150\n",
              "63      Subflow Bwd Packets   38635.386150\n",
              "65   Init_Win_bytes_forward   35616.860515\n",
              "28              Bwd IAT Min   30931.391337"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9x3SW1DzTu"
      },
      "source": [
        "Exlusão dos parâmetros que não seram usados no modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfzZxvkcEGm0",
        "outputId": "60f47bb8-cc45-4380-9245-8654e04d0a00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "LDAP.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
              "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
              "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
              "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
              "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
              "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
              "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
              "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
              "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
              "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
              "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
              "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
              "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLjyHynEVY-"
      },
      "source": [
        "x_train = x_train.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)\n",
        "\n",
        "x_test = x_test.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARPy2izzJFxW"
      },
      "source": [
        "### Normalização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oaGTiRsYjln"
      },
      "source": [
        "normalizador = MinMaxScaler()\n",
        "x_train= normalizador.fit_transform(x_train)\n",
        "x_test = normalizador.fit_transform(x_test)\n",
        "y_train = y_train.values.reshape(-1,1)\n",
        "y_test = y_test.values.reshape(-1,1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCe_uiUlpIzN"
      },
      "source": [
        "### Formatação do tensor em 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmYg8yFCIUBQ"
      },
      "source": [
        "x_train= x_train.reshape(-1, 233634, 15)\n",
        "y_train= y_train.reshape(-1, 233634, 1)\n",
        "x_test = x_test.reshape(-1, 100129, 15)\n",
        "y_test = y_test.reshape(-1, 100129, 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-eJBhktpWyk",
        "outputId": "5459e777-bf69-4173-ad62-f10ef51c3cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 233634, 15)\n",
            "(1, 233634, 1)\n",
            "(1, 100129, 15)\n",
            "(1, 100129, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4z-gkHpcYD"
      },
      "source": [
        "### Rede Neural Recorrente (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAxNKXpNuvT"
      },
      "source": [
        "#### Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_PrK6L9plVz",
        "outputId": "e40c7e30-3587-43c9-d40c-e6d7e8fd0e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(units = 20, return_sequences = True, input_shape=(233634, 15)))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "# Camada Final\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer = 'RMSprop', loss = 'mean_squared_error',\n",
        "                  metrics=['accuracy', 'AUC', 'Recall', 'Precision', 'RootMeanSquaredError'])\n",
        "# Fit the model\n",
        "model1.fit(x_train,y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2418 - accuracy: 0.7129 - auc: 0.4630 - recall: 0.7132 - precision: 0.9993 - root_mean_squared_error: 0.4917WARNING:tensorflow:Model was constructed with shape (None, 233634, 15) for input Tensor(\"lstm_input:0\", shape=(None, 233634, 15), dtype=float32), but it was called on an input with incompatible shape (None, 100129, 15).\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2418 - accuracy: 0.7129 - auc: 0.4630 - recall: 0.7132 - precision: 0.9993 - root_mean_squared_error: 0.4917 - val_loss: 0.2455 - val_accuracy: 0.9991 - val_auc: 0.5000 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4954\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2312 - accuracy: 0.8762 - auc: 0.5172 - recall: 0.8767 - precision: 0.9993 - root_mean_squared_error: 0.4809 - val_loss: 0.2419 - val_accuracy: 0.9991 - val_auc: 0.4984 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4919\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2233 - accuracy: 0.9134 - auc: 0.5061 - recall: 0.9139 - precision: 0.9993 - root_mean_squared_error: 0.4725 - val_loss: 0.2386 - val_accuracy: 0.9991 - val_auc: 0.5002 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4884\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2156 - accuracy: 0.9233 - auc: 0.4904 - recall: 0.9238 - precision: 0.9993 - root_mean_squared_error: 0.4643 - val_loss: 0.2352 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4849\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2075 - accuracy: 0.9286 - auc: 0.5250 - recall: 0.9292 - precision: 0.9994 - root_mean_squared_error: 0.4555 - val_loss: 0.2316 - val_accuracy: 0.9991 - val_auc: 0.4987 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4812\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1987 - accuracy: 0.9333 - auc: 0.4666 - recall: 0.9339 - precision: 0.9993 - root_mean_squared_error: 0.4458 - val_loss: 0.2277 - val_accuracy: 0.9991 - val_auc: 0.5022 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4772\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1888 - accuracy: 0.9373 - auc: 0.5447 - recall: 0.9379 - precision: 0.9993 - root_mean_squared_error: 0.4345 - val_loss: 0.2234 - val_accuracy: 0.9991 - val_auc: 0.4957 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4726\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1780 - accuracy: 0.9432 - auc: 0.4915 - recall: 0.9438 - precision: 0.9993 - root_mean_squared_error: 0.4219 - val_loss: 0.2184 - val_accuracy: 0.9991 - val_auc: 0.5896 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4674\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1668 - accuracy: 0.9547 - auc: 0.4799 - recall: 0.9553 - precision: 0.9993 - root_mean_squared_error: 0.4084 - val_loss: 0.2130 - val_accuracy: 0.9991 - val_auc: 0.4980 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4615\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1559 - accuracy: 0.9622 - auc: 0.4736 - recall: 0.9629 - precision: 0.9993 - root_mean_squared_error: 0.3949 - val_loss: 0.2071 - val_accuracy: 0.9991 - val_auc: 0.4992 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4551\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1460 - accuracy: 0.9665 - auc: 0.5243 - recall: 0.9671 - precision: 0.9993 - root_mean_squared_error: 0.3820 - val_loss: 0.2011 - val_accuracy: 0.9991 - val_auc: 0.5039 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4484\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1372 - accuracy: 0.9706 - auc: 0.5083 - recall: 0.9713 - precision: 0.9993 - root_mean_squared_error: 0.3704 - val_loss: 0.1949 - val_accuracy: 0.9991 - val_auc: 0.4978 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4415\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1295 - accuracy: 0.9742 - auc: 0.4391 - recall: 0.9748 - precision: 0.9993 - root_mean_squared_error: 0.3599 - val_loss: 0.1887 - val_accuracy: 0.9991 - val_auc: 0.5017 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4344\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1226 - accuracy: 0.9782 - auc: 0.5085 - recall: 0.9788 - precision: 0.9993 - root_mean_squared_error: 0.3502 - val_loss: 0.1824 - val_accuracy: 0.9991 - val_auc: 0.5585 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4271\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1166 - accuracy: 0.9821 - auc: 0.4803 - recall: 0.9827 - precision: 0.9993 - root_mean_squared_error: 0.3414 - val_loss: 0.1762 - val_accuracy: 0.9991 - val_auc: 0.5017 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4197\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1107 - accuracy: 0.9857 - auc: 0.4982 - recall: 0.9864 - precision: 0.9993 - root_mean_squared_error: 0.3328 - val_loss: 0.1699 - val_accuracy: 0.9991 - val_auc: 0.1556 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4122\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1061 - accuracy: 0.9883 - auc: 0.4719 - recall: 0.9890 - precision: 0.9993 - root_mean_squared_error: 0.3257 - val_loss: 0.1637 - val_accuracy: 0.9991 - val_auc: 0.4971 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.4046\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1014 - accuracy: 0.9901 - auc: 0.4609 - recall: 0.9908 - precision: 0.9993 - root_mean_squared_error: 0.3184 - val_loss: 0.1576 - val_accuracy: 0.9991 - val_auc: 0.5035 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3970\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0974 - accuracy: 0.9908 - auc: 0.4942 - recall: 0.9914 - precision: 0.9993 - root_mean_squared_error: 0.3121 - val_loss: 0.1516 - val_accuracy: 0.9991 - val_auc: 0.4932 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3893\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0935 - accuracy: 0.9921 - auc: 0.5011 - recall: 0.9928 - precision: 0.9993 - root_mean_squared_error: 0.3057 - val_loss: 0.1457 - val_accuracy: 0.9991 - val_auc: 0.5048 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3816\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0900 - accuracy: 0.9924 - auc: 0.4612 - recall: 0.9931 - precision: 0.9993 - root_mean_squared_error: 0.3000 - val_loss: 0.1399 - val_accuracy: 0.9991 - val_auc: 0.4950 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3740\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0868 - accuracy: 0.9928 - auc: 0.4847 - recall: 0.9935 - precision: 0.9993 - root_mean_squared_error: 0.2947 - val_loss: 0.1343 - val_accuracy: 0.9991 - val_auc: 0.5055 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3664\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0834 - accuracy: 0.9936 - auc: 0.5038 - recall: 0.9943 - precision: 0.9993 - root_mean_squared_error: 0.2889 - val_loss: 0.1288 - val_accuracy: 0.9991 - val_auc: 0.4850 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3589\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0808 - accuracy: 0.9939 - auc: 0.5134 - recall: 0.9945 - precision: 0.9993 - root_mean_squared_error: 0.2842 - val_loss: 0.1235 - val_accuracy: 0.9991 - val_auc: 0.5001 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3514\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0780 - accuracy: 0.9941 - auc: 0.4718 - recall: 0.9948 - precision: 0.9993 - root_mean_squared_error: 0.2792 - val_loss: 0.1183 - val_accuracy: 0.9991 - val_auc: 0.4908 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3440\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0754 - accuracy: 0.9948 - auc: 0.4941 - recall: 0.9955 - precision: 0.9993 - root_mean_squared_error: 0.2746 - val_loss: 0.1133 - val_accuracy: 0.9991 - val_auc: 0.4994 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3366\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0729 - accuracy: 0.9955 - auc: 0.4862 - recall: 0.9962 - precision: 0.9993 - root_mean_squared_error: 0.2700 - val_loss: 0.1085 - val_accuracy: 0.9991 - val_auc: 0.4896 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3294\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0707 - accuracy: 0.9956 - auc: 0.5048 - recall: 0.9963 - precision: 0.9993 - root_mean_squared_error: 0.2659 - val_loss: 0.1038 - val_accuracy: 0.9991 - val_auc: 0.4366 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3222\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0684 - accuracy: 0.9960 - auc: 0.4996 - recall: 0.9967 - precision: 0.9993 - root_mean_squared_error: 0.2616 - val_loss: 0.0993 - val_accuracy: 0.9991 - val_auc: 0.4984 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3151\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0661 - accuracy: 0.9963 - auc: 0.4498 - recall: 0.9970 - precision: 0.9993 - root_mean_squared_error: 0.2571 - val_loss: 0.0948 - val_accuracy: 0.9991 - val_auc: 0.4883 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3079\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0642 - accuracy: 0.9967 - auc: 0.4793 - recall: 0.9974 - precision: 0.9993 - root_mean_squared_error: 0.2534 - val_loss: 0.0906 - val_accuracy: 0.9991 - val_auc: 0.4961 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.3009\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0622 - accuracy: 0.9971 - auc: 0.4937 - recall: 0.9978 - precision: 0.9993 - root_mean_squared_error: 0.2494 - val_loss: 0.0864 - val_accuracy: 0.9991 - val_auc: 0.4867 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2940\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0601 - accuracy: 0.9975 - auc: 0.4919 - recall: 0.9982 - precision: 0.9993 - root_mean_squared_error: 0.2452 - val_loss: 0.0824 - val_accuracy: 0.9991 - val_auc: 0.3874 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2871\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0581 - accuracy: 0.9979 - auc: 0.5028 - recall: 0.9985 - precision: 0.9993 - root_mean_squared_error: 0.2410 - val_loss: 0.0785 - val_accuracy: 0.9991 - val_auc: 0.4925 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2802\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0565 - accuracy: 0.9981 - auc: 0.4719 - recall: 0.9988 - precision: 0.9993 - root_mean_squared_error: 0.2378 - val_loss: 0.0747 - val_accuracy: 0.9991 - val_auc: 0.4917 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2733\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0547 - accuracy: 0.9983 - auc: 0.4593 - recall: 0.9990 - precision: 0.9993 - root_mean_squared_error: 0.2339 - val_loss: 0.0710 - val_accuracy: 0.9991 - val_auc: 0.5000 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2665\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0527 - accuracy: 0.9985 - auc: 0.4956 - recall: 0.9992 - precision: 0.9993 - root_mean_squared_error: 0.2297 - val_loss: 0.0673 - val_accuracy: 0.9991 - val_auc: 0.4894 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2595\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0511 - accuracy: 0.9986 - auc: 0.4759 - recall: 0.9993 - precision: 0.9993 - root_mean_squared_error: 0.2261 - val_loss: 0.0638 - val_accuracy: 0.9991 - val_auc: 0.4907 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2526\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0496 - accuracy: 0.9987 - auc: 0.5045 - recall: 0.9993 - precision: 0.9993 - root_mean_squared_error: 0.2227 - val_loss: 0.0604 - val_accuracy: 0.9991 - val_auc: 0.4962 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2458\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0479 - accuracy: 0.9988 - auc: 0.4925 - recall: 0.9994 - precision: 0.9993 - root_mean_squared_error: 0.2190 - val_loss: 0.0571 - val_accuracy: 0.9991 - val_auc: 0.4868 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2390\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0466 - accuracy: 0.9989 - auc: 0.5027 - recall: 0.9996 - precision: 0.9993 - root_mean_squared_error: 0.2158 - val_loss: 0.0540 - val_accuracy: 0.9991 - val_auc: 0.4757 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2323\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0451 - accuracy: 0.9989 - auc: 0.4734 - recall: 0.9995 - precision: 0.9993 - root_mean_squared_error: 0.2124 - val_loss: 0.0510 - val_accuracy: 0.9991 - val_auc: 0.5016 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2258\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0439 - accuracy: 0.9990 - auc: 0.5170 - recall: 0.9996 - precision: 0.9993 - root_mean_squared_error: 0.2095 - val_loss: 0.0482 - val_accuracy: 0.9991 - val_auc: 0.4838 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2194\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0426 - accuracy: 0.9990 - auc: 0.5305 - recall: 0.9997 - precision: 0.9993 - root_mean_squared_error: 0.2063 - val_loss: 0.0455 - val_accuracy: 0.9991 - val_auc: 0.4879 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2134\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0413 - accuracy: 0.9991 - auc: 0.5142 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.2033 - val_loss: 0.0430 - val_accuracy: 0.9991 - val_auc: 0.4813 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2075\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0401 - accuracy: 0.9991 - auc: 0.5040 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.2004 - val_loss: 0.0407 - val_accuracy: 0.9991 - val_auc: 0.5037 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.2018\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0392 - accuracy: 0.9992 - auc: 0.4621 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1981 - val_loss: 0.0386 - val_accuracy: 0.9991 - val_auc: 0.5051 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1964\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0381 - accuracy: 0.9991 - auc: 0.5212 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1953 - val_loss: 0.0365 - val_accuracy: 0.9991 - val_auc: 0.5056 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1912\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0372 - accuracy: 0.9992 - auc: 0.4856 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1929 - val_loss: 0.0347 - val_accuracy: 0.9991 - val_auc: 0.5051 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1863\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0361 - accuracy: 0.9991 - auc: 0.5243 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1899 - val_loss: 0.0330 - val_accuracy: 0.9991 - val_auc: 0.5036 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f97166f66a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U6pSCXepu3v",
        "outputId": "9fa7cfd0-0acb-4bac-a8a6-99a81bf13d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "print(model1.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 233634, 20)        2880      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 233634, 20)        0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 233634, 10)        1240      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 233634, 1)         11        \n",
            "=================================================================\n",
            "Total params: 5,811\n",
            "Trainable params: 5,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRfi0QunpyIX",
        "outputId": "43c2ba21-5a36-456c-ccd9-def5e7b6aaa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer1 = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
        "hist1 = model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[checkpointer1], verbose = 2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.03135, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0352 - accuracy: 0.9992 - auc: 0.4913 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1875 - val_loss: 0.0314 - val_accuracy: 0.9991 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1771\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.03135 to 0.02986, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0343 - accuracy: 0.9992 - auc: 0.5030 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1853 - val_loss: 0.0299 - val_accuracy: 0.9991 - val_auc: 0.4936 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1728\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.02986 to 0.02847, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0336 - accuracy: 0.9992 - auc: 0.5173 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1832 - val_loss: 0.0285 - val_accuracy: 0.9991 - val_auc: 0.4833 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1687\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.02847 to 0.02718, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0327 - accuracy: 0.9992 - auc: 0.4918 - recall: 0.9998 - precision: 0.9993 - root_mean_squared_error: 0.1807 - val_loss: 0.0272 - val_accuracy: 0.9991 - val_auc: 0.5042 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1649\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.02718 to 0.02598, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0319 - accuracy: 0.9992 - auc: 0.4940 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1787 - val_loss: 0.0260 - val_accuracy: 0.9991 - val_auc: 0.5034 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1612\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.02598 to 0.02486, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0313 - accuracy: 0.9992 - auc: 0.4927 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1768 - val_loss: 0.0249 - val_accuracy: 0.9991 - val_auc: 0.5028 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1577\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.02486 to 0.02381, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0305 - accuracy: 0.9992 - auc: 0.4773 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1748 - val_loss: 0.0238 - val_accuracy: 0.9991 - val_auc: 0.4949 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1543\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.02381 to 0.02282, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0298 - accuracy: 0.9992 - auc: 0.4997 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1727 - val_loss: 0.0228 - val_accuracy: 0.9991 - val_auc: 0.4993 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1511\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.02282 to 0.02190, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0292 - accuracy: 0.9992 - auc: 0.5181 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1708 - val_loss: 0.0219 - val_accuracy: 0.9991 - val_auc: 0.4598 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1480\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.02190 to 0.02104, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0284 - accuracy: 0.9992 - auc: 0.5136 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1685 - val_loss: 0.0210 - val_accuracy: 0.9991 - val_auc: 0.5004 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1451\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.02104 to 0.02023, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0278 - accuracy: 0.9992 - auc: 0.5393 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1667 - val_loss: 0.0202 - val_accuracy: 0.9991 - val_auc: 0.5025 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1422\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.02023 to 0.01947, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0272 - accuracy: 0.9992 - auc: 0.5215 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1649 - val_loss: 0.0195 - val_accuracy: 0.9991 - val_auc: 0.5008 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1395\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01947 to 0.01874, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0266 - accuracy: 0.9992 - auc: 0.5465 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1630 - val_loss: 0.0187 - val_accuracy: 0.9991 - val_auc: 0.4993 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1369\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01874 to 0.01806, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0260 - accuracy: 0.9992 - auc: 0.4953 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1612 - val_loss: 0.0181 - val_accuracy: 0.9991 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1344\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01806 to 0.01741, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0254 - accuracy: 0.9993 - auc: 0.5114 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1594 - val_loss: 0.0174 - val_accuracy: 0.9991 - val_auc: 0.4993 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1320\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01741 to 0.01680, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0249 - accuracy: 0.9993 - auc: 0.5024 - recall: 0.9999 - precision: 0.9993 - root_mean_squared_error: 0.1577 - val_loss: 0.0168 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1296\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01680 to 0.01622, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0243 - accuracy: 0.9993 - auc: 0.5306 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1560 - val_loss: 0.0162 - val_accuracy: 0.9991 - val_auc: 0.4992 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1273\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01622 to 0.01567, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0239 - accuracy: 0.9993 - auc: 0.4920 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1545 - val_loss: 0.0157 - val_accuracy: 0.9991 - val_auc: 0.4995 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1252\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01567 to 0.01515, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0234 - accuracy: 0.9993 - auc: 0.4770 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1529 - val_loss: 0.0152 - val_accuracy: 0.9991 - val_auc: 0.4988 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1231\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01515 to 0.01467, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0228 - accuracy: 0.9993 - auc: 0.4694 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1511 - val_loss: 0.0147 - val_accuracy: 0.9991 - val_auc: 0.4995 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1211\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.01467 to 0.01422, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0222 - accuracy: 0.9993 - auc: 0.5208 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1491 - val_loss: 0.0142 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1192\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01422 to 0.01380, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0219 - accuracy: 0.9993 - auc: 0.5050 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1480 - val_loss: 0.0138 - val_accuracy: 0.9991 - val_auc: 0.4993 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1175\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01380 to 0.01339, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0214 - accuracy: 0.9993 - auc: 0.4947 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1462 - val_loss: 0.0134 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1157\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01339 to 0.01301, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0210 - accuracy: 0.9993 - auc: 0.5169 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1450 - val_loss: 0.0130 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1141\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01301 to 0.01265, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0206 - accuracy: 0.9993 - auc: 0.4756 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1434 - val_loss: 0.0126 - val_accuracy: 0.9991 - val_auc: 0.4994 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1125\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01265 to 0.01230, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0201 - accuracy: 0.9993 - auc: 0.4901 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1419 - val_loss: 0.0123 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1109\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01230 to 0.01196, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0197 - accuracy: 0.9993 - auc: 0.4806 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1404 - val_loss: 0.0120 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1094\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01196 to 0.01164, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0194 - accuracy: 0.9993 - auc: 0.4895 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1391 - val_loss: 0.0116 - val_accuracy: 0.9991 - val_auc: 0.4994 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1079\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01164 to 0.01133, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0190 - accuracy: 0.9993 - auc: 0.4541 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1378 - val_loss: 0.0113 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1064\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.01133 to 0.01103, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0186 - accuracy: 0.9993 - auc: 0.5082 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1365 - val_loss: 0.0110 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1050\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.01103 to 0.01074, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0182 - accuracy: 0.9993 - auc: 0.5443 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1349 - val_loss: 0.0107 - val_accuracy: 0.9991 - val_auc: 0.4994 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1036\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.01074 to 0.01045, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0179 - accuracy: 0.9993 - auc: 0.5296 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1338 - val_loss: 0.0105 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1022\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.01045 to 0.01018, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0175 - accuracy: 0.9993 - auc: 0.4648 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1322 - val_loss: 0.0102 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.1009\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.01018 to 0.00991, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0171 - accuracy: 0.9993 - auc: 0.4988 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1308 - val_loss: 0.0099 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0996\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00991 to 0.00965, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0169 - accuracy: 0.9993 - auc: 0.4888 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1302 - val_loss: 0.0097 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0983\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00965 to 0.00940, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0165 - accuracy: 0.9993 - auc: 0.5136 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1283 - val_loss: 0.0094 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0970\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00940 to 0.00916, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0162 - accuracy: 0.9993 - auc: 0.4825 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1273 - val_loss: 0.0092 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0957\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00916 to 0.00892, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0158 - accuracy: 0.9993 - auc: 0.4921 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1257 - val_loss: 0.0089 - val_accuracy: 0.9991 - val_auc: 0.4993 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0944\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00892 to 0.00868, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0156 - accuracy: 0.9993 - auc: 0.4910 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1247 - val_loss: 0.0087 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0932\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00868 to 0.00845, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0151 - accuracy: 0.9993 - auc: 0.4773 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1229 - val_loss: 0.0084 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0919\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00845 to 0.00822, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0149 - accuracy: 0.9993 - auc: 0.5034 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1222 - val_loss: 0.0082 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0907\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00822 to 0.00800, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0146 - accuracy: 0.9993 - auc: 0.5136 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1209 - val_loss: 0.0080 - val_accuracy: 0.9991 - val_auc: 0.4995 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0894\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00800 to 0.00777, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0144 - accuracy: 0.9993 - auc: 0.4735 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1199 - val_loss: 0.0078 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0882\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00777 to 0.00755, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0140 - accuracy: 0.9993 - auc: 0.5143 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1182 - val_loss: 0.0076 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0869\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00755 to 0.00734, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0136 - accuracy: 0.9993 - auc: 0.5251 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1165 - val_loss: 0.0073 - val_accuracy: 0.9991 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0856\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00734 to 0.00712, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0133 - accuracy: 0.9993 - auc: 0.5085 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1155 - val_loss: 0.0071 - val_accuracy: 0.9991 - val_auc: 0.4996 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0844\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00712 to 0.00690, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0130 - accuracy: 0.9993 - auc: 0.4854 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1140 - val_loss: 0.0069 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0831\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00690 to 0.00668, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0127 - accuracy: 0.9993 - auc: 0.5135 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1128 - val_loss: 0.0067 - val_accuracy: 0.9991 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0817\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00668 to 0.00646, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0124 - accuracy: 0.9993 - auc: 0.4997 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1114 - val_loss: 0.0065 - val_accuracy: 0.9991 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0804\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00646 to 0.00624, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0121 - accuracy: 0.9993 - auc: 0.4781 - recall: 1.0000 - precision: 0.9993 - root_mean_squared_error: 0.1101 - val_loss: 0.0062 - val_accuracy: 0.9991 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9991 - val_root_mean_squared_error: 0.0790\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTDyogpp3Pe",
        "outputId": "bb8a05df-b8b7-498f-8ad6-d18adf2c1732",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "scores1 = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print('LDAP')\n",
        "print(\"Loss: %.2f%%\" % (scores1[0]*100))\n",
        "print(\"Acurácia: %.2f%%\" % (scores1[1]*100))\n",
        "print(\"AUC: %.2f%%\" % (scores1[2]*100))\n",
        "print(\"Recall: %.2f%%\" % (scores1[3]*100))\n",
        "print(\"Precision: %.2f%%\" % (scores1[4]*100))\n",
        "print(\"RootMeanSquaredError: %.2f%%\" % (scores1[5]*100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDAP\n",
            "Loss: 0.62%\n",
            "Acurácia: 99.91%\n",
            "AUC: 49.97%\n",
            "Recall: 100.00%\n",
            "Precision: 99.91%\n",
            "RootMeanSquaredError: 7.90%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOfAv2_bJowY",
        "outputId": "a15a8239-a350-4df3-bc46-d6f9b7af941b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.title('LDAP')\n",
        "plt.plot(hist1.history['loss'], label='train')\n",
        "plt.plot(hist1.history['val_loss'], label='test')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}