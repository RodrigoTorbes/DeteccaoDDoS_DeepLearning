{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_NetBIOS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr",
        "outputId": "d67afb24-5409-4ca1-a61f-39c1ab7ba539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6SYRFm9mr-"
      },
      "source": [
        "### Carregamento arquivo de pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY7uDiB9AG57",
        "outputId": "a543d211-2850-4345-fbff-8502bf6c8a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "%run \"/content/drive/My Drive/pre_processamento_TCC.ipynb\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "CPU times: user 2min 1s, sys: 12 s, total: 2min 13s\n",
            "Wall time: 2min 19s\n",
            "Ataque de exploração UDPLag:  Label\n",
            "BENIGN       3705\n",
            "UDP-lag    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "BENIGN        392\n",
            "Syn       1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "BENIGN           1612\n",
            "DrDoS_LDAP    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "BENIGN              1707\n",
            "DrDoS_NetBIOS    4093279\n",
            "dtype: int64\n",
            "Ataque de exploração UDPLag:  Label\n",
            "0      3705\n",
            "1    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "0        392\n",
            "1    1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "0       1612\n",
            "1    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "0       1707\n",
            "1    4093279\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ph-Q5_JRBAq6",
        "outputId": "21025e71-fa46-4229-f15a-4d70968f8e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "netbios = netbios.sample(333763)\n",
        "print(netbios.groupby(by=' Label').size())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Label\n",
            "0       157\n",
            "1    333606\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXdSSkdaH4WQ"
      },
      "source": [
        "### Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyNbLHJIBlJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWBHgHzHU9w"
      },
      "source": [
        "### Divisão do conjunto em treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53UJNEuJCQf",
        "outputId": "adaf0bb0-c820-4c65-8304-9aa4dba27fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "netbios"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Fwd Avg Bytes/Bulk</th>\n",
              "      <th>Fwd Avg Packets/Bulk</th>\n",
              "      <th>Fwd Avg Bulk Rate</th>\n",
              "      <th>Bwd Avg Bytes/Bulk</th>\n",
              "      <th>Bwd Avg Packets/Bulk</th>\n",
              "      <th>Bwd Avg Bulk Rate</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>915225</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>424.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>212000000.0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>1000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>318.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>424</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3404077</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2752253</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602365</th>\n",
              "      <td>50</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>422.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8440000.0</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.5</td>\n",
              "      <td>211.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>422</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3194500</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3086159</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3387432</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1851852</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>494.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>494000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>247.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>370.5</td>\n",
              "      <td>247.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>494</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2203672</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2272856</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>458.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>458000000.0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2125437950</td>\n",
              "      <td>0</td>\n",
              "      <td>2000000.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>343.5</td>\n",
              "      <td>229.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2125437950</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>458</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1062718975</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333763 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          Flow Duration   Total Fwd Packets  ...   Idle Min   Label\n",
              "915225                2                   2  ...        0.0       1\n",
              "3404077               1                   2  ...        0.0       1\n",
              "2752253               1                   2  ...        0.0       1\n",
              "602365               50                   2  ...        0.0       1\n",
              "3194500               1                   2  ...        0.0       1\n",
              "...                 ...                 ...  ...        ...     ...\n",
              "3086159               1                   2  ...        0.0       1\n",
              "3387432               1                   2  ...        0.0       1\n",
              "1851852               1                   2  ...        0.0       1\n",
              "2203672               1                   2  ...        0.0       1\n",
              "2272856               1                   2  ...        0.0       1\n",
              "\n",
              "[333763 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ldV1PAMGk6h"
      },
      "source": [
        "Preparação dos dados treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JQDSHBGMm1"
      },
      "source": [
        "X = netbios.iloc[:, 0:77]\n",
        "y = netbios.iloc[:,- 1]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1hcfSsGQEj"
      },
      "source": [
        "70% para treino, 30% para teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmiwVL7bDRaL"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWSoJT6XFgyZ",
        "outputId": "2d605bbc-c232-43a6-f7f7-98b033f61143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Formato dos dados de entrada\n",
        "print('Formato dos dados de entrada:', x_train.shape)\n",
        "\n",
        "# Tamanho dos conjuntos\n",
        "print('Amostras de treino: ', x_train.shape[0])\n",
        "print('Amostras de teste: ', x_test.shape[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Formato dos dados de entrada: (233634, 77)\n",
            "Amostras de treino:  233634\n",
            "Amostras de teste:  100129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGh7k5c6Hd2e"
      },
      "source": [
        "### Seleção dos Parâmetro\n",
        "Seleção dos 15 melhores parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UNx2xDtDg6K",
        "outputId": "88b8147e-a153-4105-a1d1-992d13a1a324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "best_features = SelectKBest(score_func=f_classif, k=15)\n",
        "fit = best_features.fit(x_train,y_train)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(x_train.columns)\n",
        "# concatenar quadros de dados\n",
        "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "feature_scores.columns = ['Feature_Name','Score']  # colunas de saída de nome\n",
        "print(feature_scores.nlargest(15,'Score'))  # imprima 15 melhores parâmetros"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               Feature_Name          Score\n",
            "50            Down/Up Ratio  136811.178534\n",
            "47           URG Flag Count  112717.010046\n",
            "10    Bwd Packet Length Min  109880.081766\n",
            "11   Bwd Packet Length Mean   42909.494539\n",
            "53     Avg Bwd Segment Size   42909.494539\n",
            "40        Packet Length Std   40615.447896\n",
            "29            Fwd PSH Flags   40098.707241\n",
            "44           RST Flag Count   40098.707241\n",
            "9     Bwd Packet Length Max   36934.088013\n",
            "2    Total Backward Packets   34716.875017\n",
            "63      Subflow Bwd Packets   34716.875017\n",
            "25             Bwd IAT Mean   26968.597036\n",
            "12    Bwd Packet Length Std   25663.971229\n",
            "41   Packet Length Variance   25023.659271\n",
            "26              Bwd IAT Std   23763.536935\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [30 31 32 42 43 45 49 55 56 57 58 59 60] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TRpDjFn-OC",
        "outputId": "56bb5e21-3b43-421b-f023-3425b27eec29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "feature = feature_scores.nlargest(15,'Score')\n",
        "feature"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature_Name</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Down/Up Ratio</td>\n",
              "      <td>136811.178534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>URG Flag Count</td>\n",
              "      <td>112717.010046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Bwd Packet Length Min</td>\n",
              "      <td>109880.081766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Bwd Packet Length Mean</td>\n",
              "      <td>42909.494539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Avg Bwd Segment Size</td>\n",
              "      <td>42909.494539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Packet Length Std</td>\n",
              "      <td>40615.447896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Fwd PSH Flags</td>\n",
              "      <td>40098.707241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>RST Flag Count</td>\n",
              "      <td>40098.707241</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bwd Packet Length Max</td>\n",
              "      <td>36934.088013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Total Backward Packets</td>\n",
              "      <td>34716.875017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>Subflow Bwd Packets</td>\n",
              "      <td>34716.875017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Bwd IAT Mean</td>\n",
              "      <td>26968.597036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Bwd Packet Length Std</td>\n",
              "      <td>25663.971229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Packet Length Variance</td>\n",
              "      <td>25023.659271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Bwd IAT Std</td>\n",
              "      <td>23763.536935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Feature_Name          Score\n",
              "50            Down/Up Ratio  136811.178534\n",
              "47           URG Flag Count  112717.010046\n",
              "10    Bwd Packet Length Min  109880.081766\n",
              "11   Bwd Packet Length Mean   42909.494539\n",
              "53     Avg Bwd Segment Size   42909.494539\n",
              "40        Packet Length Std   40615.447896\n",
              "29            Fwd PSH Flags   40098.707241\n",
              "44           RST Flag Count   40098.707241\n",
              "9     Bwd Packet Length Max   36934.088013\n",
              "2    Total Backward Packets   34716.875017\n",
              "63      Subflow Bwd Packets   34716.875017\n",
              "25             Bwd IAT Mean   26968.597036\n",
              "12    Bwd Packet Length Std   25663.971229\n",
              "41   Packet Length Variance   25023.659271\n",
              "26              Bwd IAT Std   23763.536935"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9x3SW1DzTu"
      },
      "source": [
        "Exlusão dos parâmetros que não seram usados no modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfzZxvkcEGm0",
        "outputId": "04229bf3-ff27-4c03-94ab-701b6d99cac4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "netbios.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
              "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
              "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
              "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
              "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
              "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
              "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
              "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
              "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
              "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
              "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
              "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
              "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLjyHynEVY-"
      },
      "source": [
        "x_train = x_train.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)\n",
        "\n",
        "x_test = x_test.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARPy2izzJFxW"
      },
      "source": [
        "### Normalização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oaGTiRsYjln"
      },
      "source": [
        "normalizador = MinMaxScaler()\n",
        "x_train= normalizador.fit_transform(x_train)\n",
        "x_test = normalizador.fit_transform(x_test)\n",
        "y_train = y_train.values.reshape(-1,1)\n",
        "y_test = y_test.values.reshape(-1,1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCe_uiUlpIzN"
      },
      "source": [
        "### Formatação do tensor em 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmYg8yFCIUBQ"
      },
      "source": [
        "x_train= x_train.reshape(-1, 233634, 15)\n",
        "y_train= y_train.reshape(-1, 233634, 1)\n",
        "x_test = x_test.reshape(-1, 100129, 15)\n",
        "y_test = y_test.reshape(-1, 100129, 1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-eJBhktpWyk",
        "outputId": "dc5e2411-306a-4a68-d8b3-72a8a3e3c70a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 233634, 15)\n",
            "(1, 233634, 1)\n",
            "(1, 100129, 15)\n",
            "(1, 100129, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4z-gkHpcYD"
      },
      "source": [
        "### Rede Neural Recorrente (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAxNKXpNuvT"
      },
      "source": [
        "#### Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_PrK6L9plVz",
        "outputId": "f1fc71f0-421a-46d3-fb89-590e33df3204",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model1 = Sequential()\n",
        "model1.add(LSTM(units = 20, return_sequences = True, input_shape=(233634, 15)))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "# Camada Final\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer = 'RMSprop', loss = 'mean_squared_error',\n",
        "                  metrics=['accuracy', 'AUC', 'Recall', 'Precision', 'RootMeanSquaredError'])\n",
        "# Fit the model\n",
        "model1.fit(x_train,y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2465 - accuracy: 0.7106 - auc: 0.4949 - recall: 0.7108 - precision: 0.9995 - root_mean_squared_error: 0.4965WARNING:tensorflow:Model was constructed with shape (None, 233634, 15) for input Tensor(\"lstm_input:0\", shape=(None, 233634, 15), dtype=float32), but it was called on an input with incompatible shape (None, 100129, 15).\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2465 - accuracy: 0.7106 - auc: 0.4949 - recall: 0.7108 - precision: 0.9995 - root_mean_squared_error: 0.4965 - val_loss: 0.2284 - val_accuracy: 0.9996 - val_auc: 0.5005 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.4780\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2290 - accuracy: 0.9672 - auc: 0.4798 - recall: 0.9677 - precision: 0.9995 - root_mean_squared_error: 0.4785 - val_loss: 0.2101 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.4584\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2115 - accuracy: 0.9832 - auc: 0.4948 - recall: 0.9837 - precision: 0.9995 - root_mean_squared_error: 0.4599 - val_loss: 0.1901 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.4360\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1931 - accuracy: 0.9895 - auc: 0.5038 - recall: 0.9900 - precision: 0.9995 - root_mean_squared_error: 0.4395 - val_loss: 0.1704 - val_accuracy: 0.9996 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.4128\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1752 - accuracy: 0.9936 - auc: 0.5247 - recall: 0.9941 - precision: 0.9995 - root_mean_squared_error: 0.4185 - val_loss: 0.1528 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3909\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1592 - accuracy: 0.9960 - auc: 0.5031 - recall: 0.9965 - precision: 0.9995 - root_mean_squared_error: 0.3991 - val_loss: 0.1376 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3709\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1453 - accuracy: 0.9973 - auc: 0.4824 - recall: 0.9978 - precision: 0.9995 - root_mean_squared_error: 0.3812 - val_loss: 0.1244 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3526\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1334 - accuracy: 0.9980 - auc: 0.5306 - recall: 0.9985 - precision: 0.9995 - root_mean_squared_error: 0.3653 - val_loss: 0.1129 - val_accuracy: 0.9996 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3359\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1227 - accuracy: 0.9985 - auc: 0.5277 - recall: 0.9990 - precision: 0.9995 - root_mean_squared_error: 0.3504 - val_loss: 0.1027 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3205\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1132 - accuracy: 0.9988 - auc: 0.5203 - recall: 0.9993 - precision: 0.9995 - root_mean_squared_error: 0.3365 - val_loss: 0.0937 - val_accuracy: 0.9996 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.3060\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1047 - accuracy: 0.9990 - auc: 0.5331 - recall: 0.9995 - precision: 0.9995 - root_mean_squared_error: 0.3236 - val_loss: 0.0855 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2924\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0969 - accuracy: 0.9991 - auc: 0.5256 - recall: 0.9996 - precision: 0.9995 - root_mean_squared_error: 0.3113 - val_loss: 0.0782 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2796\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0897 - accuracy: 0.9992 - auc: 0.4640 - recall: 0.9997 - precision: 0.9995 - root_mean_squared_error: 0.2996 - val_loss: 0.0715 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2674\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0831 - accuracy: 0.9993 - auc: 0.5271 - recall: 0.9998 - precision: 0.9995 - root_mean_squared_error: 0.2882 - val_loss: 0.0655 - val_accuracy: 0.9996 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2558\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0770 - accuracy: 0.9994 - auc: 0.5185 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2776 - val_loss: 0.0600 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2449\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0714 - accuracy: 0.9994 - auc: 0.5089 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2671 - val_loss: 0.0550 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2345\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0665 - accuracy: 0.9994 - auc: 0.4831 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2579 - val_loss: 0.0505 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2247\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0617 - accuracy: 0.9994 - auc: 0.4643 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2483 - val_loss: 0.0465 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2155\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0574 - accuracy: 0.9994 - auc: 0.5007 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2395 - val_loss: 0.0428 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.2069\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0534 - accuracy: 0.9994 - auc: 0.5048 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2311 - val_loss: 0.0396 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1989\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0500 - accuracy: 0.9994 - auc: 0.4854 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2235 - val_loss: 0.0366 - val_accuracy: 0.9996 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1914\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0467 - accuracy: 0.9994 - auc: 0.4448 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2161 - val_loss: 0.0340 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1844\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0438 - accuracy: 0.9995 - auc: 0.5225 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.2092 - val_loss: 0.0316 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1779\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0412 - accuracy: 0.9994 - auc: 0.5041 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.2030 - val_loss: 0.0295 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1718\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0388 - accuracy: 0.9995 - auc: 0.4986 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1969 - val_loss: 0.0276 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1662\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0367 - accuracy: 0.9994 - auc: 0.4995 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.1915 - val_loss: 0.0259 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1609\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0347 - accuracy: 0.9995 - auc: 0.4928 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1864 - val_loss: 0.0243 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1560\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0329 - accuracy: 0.9995 - auc: 0.4657 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1813 - val_loss: 0.0229 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1514\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0313 - accuracy: 0.9995 - auc: 0.5517 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.1769 - val_loss: 0.0216 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1471\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0298 - accuracy: 0.9995 - auc: 0.5147 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1726 - val_loss: 0.0205 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1431\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0284 - accuracy: 0.9995 - auc: 0.4649 - recall: 0.9999 - precision: 0.9995 - root_mean_squared_error: 0.1687 - val_loss: 0.0194 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1393\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0272 - accuracy: 0.9995 - auc: 0.4976 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1650 - val_loss: 0.0184 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1358\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0262 - accuracy: 0.9995 - auc: 0.5137 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1617 - val_loss: 0.0175 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1324\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0250 - accuracy: 0.9995 - auc: 0.4847 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1582 - val_loss: 0.0167 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1293\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0241 - accuracy: 0.9995 - auc: 0.5132 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1552 - val_loss: 0.0160 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1263\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0232 - accuracy: 0.9995 - auc: 0.5526 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1524 - val_loss: 0.0153 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1235\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0224 - accuracy: 0.9995 - auc: 0.4992 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1496 - val_loss: 0.0146 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1208\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0216 - accuracy: 0.9995 - auc: 0.5238 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1469 - val_loss: 0.0140 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1183\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0209 - accuracy: 0.9995 - auc: 0.4672 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1445 - val_loss: 0.0134 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1159\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0202 - accuracy: 0.9995 - auc: 0.4936 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1422 - val_loss: 0.0129 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1135\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0196 - accuracy: 0.9995 - auc: 0.5089 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1400 - val_loss: 0.0124 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1113\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0189 - accuracy: 0.9995 - auc: 0.4813 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1374 - val_loss: 0.0119 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1092\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0185 - accuracy: 0.9995 - auc: 0.5067 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1361 - val_loss: 0.0115 - val_accuracy: 0.9996 - val_auc: 0.4983 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1071\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0179 - accuracy: 0.9995 - auc: 0.5000 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1337 - val_loss: 0.0111 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1051\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0173 - accuracy: 0.9995 - auc: 0.4549 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1314 - val_loss: 0.0107 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1032\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0168 - accuracy: 0.9995 - auc: 0.4619 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1297 - val_loss: 0.0103 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.1013\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0164 - accuracy: 0.9995 - auc: 0.4488 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1280 - val_loss: 0.0099 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0995\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0159 - accuracy: 0.9995 - auc: 0.4456 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1262 - val_loss: 0.0096 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0978\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0155 - accuracy: 0.9995 - auc: 0.5303 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1244 - val_loss: 0.0092 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0961\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0151 - accuracy: 0.9995 - auc: 0.4791 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1230 - val_loss: 0.0089 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0944\n",
            "CPU times: user 20min 40s, sys: 54.6 s, total: 21min 35s\n",
            "Wall time: 21min 42s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U6pSCXepu3v",
        "outputId": "2ee01c52-df8c-44db-d058-5f95efee54e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(model1.summary())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 233634, 20)        2880      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 233634, 20)        0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 233634, 10)        1240      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 233634, 1)         11        \n",
            "=================================================================\n",
            "Total params: 5,811\n",
            "Trainable params: 5,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRfi0QunpyIX",
        "outputId": "bd09d56f-a907-4df2-b3bf-ff1f5a33b259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer1 = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
        "hist1 = model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[checkpointer1], verbose = 2)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00862, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0148 - accuracy: 0.9995 - auc: 0.4783 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1215 - val_loss: 0.0086 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0928\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00862 to 0.00833, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0143 - accuracy: 0.9995 - auc: 0.4831 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1196 - val_loss: 0.0083 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0913\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00833 to 0.00805, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0139 - accuracy: 0.9995 - auc: 0.4938 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1178 - val_loss: 0.0081 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0897\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00805 to 0.00779, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0135 - accuracy: 0.9995 - auc: 0.5210 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1164 - val_loss: 0.0078 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0883\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00779 to 0.00754, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0132 - accuracy: 0.9995 - auc: 0.4745 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1150 - val_loss: 0.0075 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0868\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00754 to 0.00729, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0129 - accuracy: 0.9995 - auc: 0.4806 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1137 - val_loss: 0.0073 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0854\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00729 to 0.00706, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0126 - accuracy: 0.9995 - auc: 0.5089 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1124 - val_loss: 0.0071 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0840\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00706 to 0.00684, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0123 - accuracy: 0.9995 - auc: 0.5320 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1111 - val_loss: 0.0068 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0827\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00684 to 0.00662, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0121 - accuracy: 0.9995 - auc: 0.4522 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1098 - val_loss: 0.0066 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0814\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00662 to 0.00641, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0118 - accuracy: 0.9995 - auc: 0.5342 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1084 - val_loss: 0.0064 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0801\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00641 to 0.00621, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0114 - accuracy: 0.9995 - auc: 0.4697 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1069 - val_loss: 0.0062 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0788\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00621 to 0.00602, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0112 - accuracy: 0.9995 - auc: 0.4713 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1058 - val_loss: 0.0060 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0776\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00602 to 0.00583, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0109 - accuracy: 0.9995 - auc: 0.4853 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1046 - val_loss: 0.0058 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0763\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00583 to 0.00565, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0107 - accuracy: 0.9995 - auc: 0.4466 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1037 - val_loss: 0.0056 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0751\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00565 to 0.00547, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0105 - accuracy: 0.9995 - auc: 0.4923 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1022 - val_loss: 0.0055 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0740\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00547 to 0.00530, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0102 - accuracy: 0.9995 - auc: 0.5262 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.1009 - val_loss: 0.0053 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0728\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00530 to 0.00513, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0100 - accuracy: 0.9995 - auc: 0.5117 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0999 - val_loss: 0.0051 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0716\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.00513 to 0.00497, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0098 - accuracy: 0.9995 - auc: 0.4666 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0988 - val_loss: 0.0050 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0705\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00497 to 0.00481, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0095 - accuracy: 0.9995 - auc: 0.4847 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0973 - val_loss: 0.0048 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0694\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.00481 to 0.00466, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0093 - accuracy: 0.9995 - auc: 0.4555 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0965 - val_loss: 0.0047 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0683\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.00466 to 0.00451, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0091 - accuracy: 0.9995 - auc: 0.5002 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0952 - val_loss: 0.0045 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0672\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.00451 to 0.00436, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0089 - accuracy: 0.9995 - auc: 0.4829 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0941 - val_loss: 0.0044 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0660\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00436 to 0.00422, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0087 - accuracy: 0.9995 - auc: 0.4773 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0933 - val_loss: 0.0042 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0649\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.00422 to 0.00408, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0084 - accuracy: 0.9995 - auc: 0.5358 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0919 - val_loss: 0.0041 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0639\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.00408 to 0.00394, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0082 - accuracy: 0.9995 - auc: 0.5162 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0908 - val_loss: 0.0039 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0628\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.00394 to 0.00381, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0080 - accuracy: 0.9995 - auc: 0.4864 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0895 - val_loss: 0.0038 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0617\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00381 to 0.00368, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0078 - accuracy: 0.9995 - auc: 0.4693 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0884 - val_loss: 0.0037 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0606\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.00368 to 0.00355, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0076 - accuracy: 0.9995 - auc: 0.4951 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0874 - val_loss: 0.0035 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0596\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00355 to 0.00343, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0074 - accuracy: 0.9995 - auc: 0.4648 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0860 - val_loss: 0.0034 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0585\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.00343 to 0.00331, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0073 - accuracy: 0.9995 - auc: 0.4590 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0852 - val_loss: 0.0033 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0575\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.00331 to 0.00319, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0071 - accuracy: 0.9995 - auc: 0.4956 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0843 - val_loss: 0.0032 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0565\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.00319 to 0.00308, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0069 - accuracy: 0.9995 - auc: 0.5203 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0831 - val_loss: 0.0031 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0555\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.00308 to 0.00298, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0067 - accuracy: 0.9995 - auc: 0.4785 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0816 - val_loss: 0.0030 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0545\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.00298 to 0.00287, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0066 - accuracy: 0.9995 - auc: 0.4641 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0810 - val_loss: 0.0029 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0536\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.00287 to 0.00278, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0064 - accuracy: 0.9995 - auc: 0.5397 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0798 - val_loss: 0.0028 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0527\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00278 to 0.00268, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0061 - accuracy: 0.9995 - auc: 0.5575 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0783 - val_loss: 0.0027 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0518\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.00268 to 0.00259, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0061 - accuracy: 0.9995 - auc: 0.5064 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0778 - val_loss: 0.0026 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0509\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.00259 to 0.00251, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0059 - accuracy: 0.9995 - auc: 0.4743 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0769 - val_loss: 0.0025 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0501\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.00251 to 0.00243, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0057 - accuracy: 0.9995 - auc: 0.4838 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0757 - val_loss: 0.0024 - val_accuracy: 0.9996 - val_auc: 0.4998 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0493\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.00243 to 0.00235, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0056 - accuracy: 0.9995 - auc: 0.5046 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0748 - val_loss: 0.0023 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0485\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.00235 to 0.00228, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0054 - accuracy: 0.9995 - auc: 0.4791 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0738 - val_loss: 0.0023 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0477\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.00228 to 0.00221, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0053 - accuracy: 0.9995 - auc: 0.5153 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0730 - val_loss: 0.0022 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0470\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.00221 to 0.00214, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0052 - accuracy: 0.9995 - auc: 0.5029 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0722 - val_loss: 0.0021 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0462\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.00214 to 0.00208, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0051 - accuracy: 0.9995 - auc: 0.5116 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0711 - val_loss: 0.0021 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0456\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.00208 to 0.00201, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0050 - accuracy: 0.9995 - auc: 0.4999 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0705 - val_loss: 0.0020 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0449\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.00201 to 0.00196, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0048 - accuracy: 0.9995 - auc: 0.4647 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0696 - val_loss: 0.0020 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0442\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.00196 to 0.00190, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0047 - accuracy: 0.9995 - auc: 0.4828 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0687 - val_loss: 0.0019 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0436\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.00190 to 0.00185, saving model to model.weights.best.hdf5\n",
            "1/1 - 4s - loss: 0.0046 - accuracy: 0.9995 - auc: 0.4547 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0680 - val_loss: 0.0018 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0430\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.00185 to 0.00180, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0045 - accuracy: 0.9995 - auc: 0.4938 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0670 - val_loss: 0.0018 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0424\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.00180 to 0.00175, saving model to model.weights.best.hdf5\n",
            "1/1 - 5s - loss: 0.0044 - accuracy: 0.9995 - auc: 0.5517 - recall: 1.0000 - precision: 0.9995 - root_mean_squared_error: 0.0662 - val_loss: 0.0017 - val_accuracy: 0.9996 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9996 - val_root_mean_squared_error: 0.0418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paTDyogpp3Pe",
        "outputId": "eac23848-4179-4693-c4b7-719aafa2d5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "scores1 = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print('LDAP')\n",
        "print(\"Loss: %.2f%%\" % (scores1[0]*100))\n",
        "print(\"Acurácia: %.2f%%\" % (scores1[1]*100))\n",
        "print(\"AUC: %.2f%%\" % (scores1[2]*100))\n",
        "print(\"Recall: %.2f%%\" % (scores1[3]*100))\n",
        "print(\"Precision: %.2f%%\" % (scores1[4]*100))\n",
        "print(\"RootMeanSquaredError: %.2f%%\" % (scores1[5]*100))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LDAP\n",
            "Loss: 0.17%\n",
            "Acurácia: 99.96%\n",
            "AUC: 49.99%\n",
            "Recall: 100.00%\n",
            "Precision: 99.96%\n",
            "RootMeanSquaredError: 4.18%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOfAv2_bJowY",
        "outputId": "cdb15dc4-428a-4644-cf02-e4376e83705b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.title('NetBIOS')\n",
        "plt.plot(hist1.history['loss'], label='train')\n",
        "plt.plot(hist1.history['val_loss'], label='test')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}