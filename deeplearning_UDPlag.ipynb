{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_UDPlag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr",
        "outputId": "2404d988-1a49-4844-f532-47a7056fc347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6SYRFm9mr-"
      },
      "source": [
        "### Carregamento arquivo de pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY7uDiB9AG57",
        "outputId": "d01b6aaf-8ef2-467f-94ef-5f05389e30ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        }
      },
      "source": [
        "%run \"/content/drive/My Drive/Colab Notebooks/pre_processamento_TCC.ipynb\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "CPU times: user 2min 1s, sys: 11.6 s, total: 2min 13s\n",
            "Wall time: 2min 20s\n",
            "Ataque de exploração UDPLag:  Label\n",
            "BENIGN       3705\n",
            "UDP-lag    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "BENIGN        392\n",
            "Syn       1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "BENIGN           1612\n",
            "DrDoS_LDAP    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "BENIGN              1707\n",
            "DrDoS_NetBIOS    4093279\n",
            "dtype: int64\n",
            "Ataque de exploração UDPLag:  Label\n",
            "0      3705\n",
            "1    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "0        392\n",
            "1    1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "0       1612\n",
            "1    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "0       1707\n",
            "1    4093279\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXdSSkdaH4WQ"
      },
      "source": [
        "### Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyNbLHJIBlJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWBHgHzHU9w"
      },
      "source": [
        "### Divisão do conjunto em treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53UJNEuJCQf",
        "outputId": "4d6d0d0f-30cf-4156-a4a2-4b1aa088ac2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "UDPlag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Fwd Avg Bytes/Bulk</th>\n",
              "      <th>Fwd Avg Packets/Bulk</th>\n",
              "      <th>Fwd Avg Bulk Rate</th>\n",
              "      <th>Bwd Avg Bytes/Bulk</th>\n",
              "      <th>Bwd Avg Packets/Bulk</th>\n",
              "      <th>Bwd Avg Bulk Rate</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>766.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>766000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>574.5</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>766</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>778.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>778000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>583.5</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375000000.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>562.5</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>738.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369000000.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>553.5</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>738</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>750000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>562.5</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370599</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370600</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370601</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370602</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370604</th>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985075e+04</td>\n",
              "      <td>44.666667</td>\n",
              "      <td>75.632885</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "      <td>1.492537e+04</td>\n",
              "      <td>14925.373134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333763 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Flow Duration   Total Fwd Packets  ...   Idle Min   Label\n",
              "0                    1                   2  ...        0.0       1\n",
              "1                    1                   2  ...        0.0       1\n",
              "2                    2                   2  ...        0.0       1\n",
              "3                    2                   2  ...        0.0       1\n",
              "4                    1                   2  ...        0.0       1\n",
              "...                ...                 ...  ...        ...     ...\n",
              "370599               1                   2  ...        0.0       1\n",
              "370600               1                   2  ...        0.0       1\n",
              "370601               1                   2  ...        0.0       1\n",
              "370602               2                   2  ...        0.0       1\n",
              "370604             134                   2  ...        0.0       1\n",
              "\n",
              "[333763 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ldV1PAMGk6h"
      },
      "source": [
        "Preparação dos dados treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JQDSHBGMm1"
      },
      "source": [
        "X = UDPlag.iloc[:, 0:77]\n",
        "y = UDPlag.iloc[:,- 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1hcfSsGQEj"
      },
      "source": [
        "70% para treino, 30% para teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmiwVL7bDRaL"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWSoJT6XFgyZ",
        "outputId": "65424857-a4eb-48d0-e088-46fb570084ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# Formato dos dados de entrada\n",
        "print('Formato dos dados de entrada:', x_train.shape)\n",
        "\n",
        "# Tamanho dos conjuntos\n",
        "print('Amostras de treino: ', x_train.shape[0])\n",
        "print('Amostras de teste: ', x_test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Formato dos dados de entrada: (233634, 77)\n",
            "Amostras de treino:  233634\n",
            "Amostras de teste:  100129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGh7k5c6Hd2e"
      },
      "source": [
        "### Seleção dos Parâmetro\n",
        "Seleção dos 15 melhores parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UNx2xDtDg6K",
        "outputId": "1d078ed6-b337-419c-e56a-a530a311376e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "best_features = SelectKBest(score_func=f_classif, k=15)\n",
        "fit = best_features.fit(x_train,y_train)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(x_train.columns)\n",
        "# concatenar quadros de dados\n",
        "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "feature_scores.columns = ['Feature_Name','Score']  # colunas de saída de nome\n",
        "print(feature_scores.nlargest(15,'Score'))  # imprima 15 melhores parâmetros"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Feature_Name          Score\n",
            "47            URG Flag Count  110569.603753\n",
            "48            CWE Flag Count   50096.792295\n",
            "11    Bwd Packet Length Mean   33925.850698\n",
            "53      Avg Bwd Segment Size   33925.850698\n",
            "9      Bwd Packet Length Max   32796.113276\n",
            "29             Fwd PSH Flags   29159.350848\n",
            "44            RST Flag Count   29159.350848\n",
            "50             Down/Up Ratio   27909.522885\n",
            "40         Packet Length Std   26857.407173\n",
            "66   Init_Win_bytes_backward   26760.422368\n",
            "12     Bwd Packet Length Std   23563.411565\n",
            "34         Bwd Header Length   23133.283185\n",
            "8      Fwd Packet Length Std   18141.562396\n",
            "10     Bwd Packet Length Min   17837.128675\n",
            "41    Packet Length Variance   15762.971567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [30 31 32 42 45 49 55 56 57 58 59 60] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TRpDjFn-OC",
        "outputId": "c027f80f-51eb-47da-a92c-3ad62bc9ce31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "feature = feature_scores.nlargest(15,'Score')\n",
        "feature"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature_Name</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>URG Flag Count</td>\n",
              "      <td>110569.603753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>CWE Flag Count</td>\n",
              "      <td>50096.792295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Bwd Packet Length Mean</td>\n",
              "      <td>33925.850698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Avg Bwd Segment Size</td>\n",
              "      <td>33925.850698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bwd Packet Length Max</td>\n",
              "      <td>32796.113276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Fwd PSH Flags</td>\n",
              "      <td>29159.350848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>RST Flag Count</td>\n",
              "      <td>29159.350848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Down/Up Ratio</td>\n",
              "      <td>27909.522885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Packet Length Std</td>\n",
              "      <td>26857.407173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Init_Win_bytes_backward</td>\n",
              "      <td>26760.422368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Bwd Packet Length Std</td>\n",
              "      <td>23563.411565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Bwd Header Length</td>\n",
              "      <td>23133.283185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Fwd Packet Length Std</td>\n",
              "      <td>18141.562396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Bwd Packet Length Min</td>\n",
              "      <td>17837.128675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Packet Length Variance</td>\n",
              "      <td>15762.971567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Feature_Name          Score\n",
              "47            URG Flag Count  110569.603753\n",
              "48            CWE Flag Count   50096.792295\n",
              "11    Bwd Packet Length Mean   33925.850698\n",
              "53      Avg Bwd Segment Size   33925.850698\n",
              "9      Bwd Packet Length Max   32796.113276\n",
              "29             Fwd PSH Flags   29159.350848\n",
              "44            RST Flag Count   29159.350848\n",
              "50             Down/Up Ratio   27909.522885\n",
              "40         Packet Length Std   26857.407173\n",
              "66   Init_Win_bytes_backward   26760.422368\n",
              "12     Bwd Packet Length Std   23563.411565\n",
              "34         Bwd Header Length   23133.283185\n",
              "8      Fwd Packet Length Std   18141.562396\n",
              "10     Bwd Packet Length Min   17837.128675\n",
              "41    Packet Length Variance   15762.971567"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9x3SW1DzTu"
      },
      "source": [
        "Exlusão dos parâmetros que não seram usados no modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfzZxvkcEGm0",
        "outputId": "916d4bab-93bb-4bd3-b9b5-48e9e578266b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "netbios.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
              "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
              "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
              "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
              "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
              "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
              "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
              "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
              "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
              "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
              "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
              "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
              "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLjyHynEVY-"
      },
      "source": [
        "x_train = x_train.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)\n",
        "\n",
        "x_test = x_test.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARPy2izzJFxW"
      },
      "source": [
        "### Normalização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oaGTiRsYjln"
      },
      "source": [
        "normalizador = MinMaxScaler()\n",
        "x_train= normalizador.fit_transform(x_train)\n",
        "x_test = normalizador.fit_transform(x_test)\n",
        "y_train = y_train.values.reshape(-1,1)\n",
        "y_test = y_test.values.reshape(-1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCe_uiUlpIzN"
      },
      "source": [
        "### Formatação do tensor em 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmYg8yFCIUBQ"
      },
      "source": [
        "x_train= x_train.reshape(-1, 233634, 15)\n",
        "y_train= y_train.reshape(-1, 233634, 1)\n",
        "x_test = x_test.reshape(-1, 100129, 15)\n",
        "y_test = y_test.reshape(-1, 100129, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-eJBhktpWyk",
        "outputId": "78e2a25d-acf6-4787-e562-b90f8a6612f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 233634, 15)\n",
            "(1, 233634, 1)\n",
            "(1, 100129, 15)\n",
            "(1, 100129, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4z-gkHpcYD"
      },
      "source": [
        "### Rede Neural Recorrente (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAxNKXpNuvT"
      },
      "source": [
        "#### Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_PrK6L9plVz",
        "outputId": "81f54f16-ac5d-45ab-c6c1-ecef11533551",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(units = 20, return_sequences = True, input_shape=(233634, 15)))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "# Camada Final\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer = 'RMSprop', loss = 'mean_squared_error',\n",
        "                  metrics=['accuracy', 'AUC', 'Recall', 'Precision', 'RootMeanSquaredError'])\n",
        "# Fit the model\n",
        "model1.fit(x_train,y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.4425 - auc: 0.5004 - recall: 0.4412 - precision: 0.9891 - root_mean_squared_error: 0.5000WARNING:tensorflow:Model was constructed with shape (None, 233634, 15) for input Tensor(\"lstm_input:0\", shape=(None, 233634, 15), dtype=float32), but it was called on an input with incompatible shape (None, 100129, 15).\n",
            "1/1 [==============================] - 7s 7s/step - loss: 0.2500 - accuracy: 0.4425 - auc: 0.5004 - recall: 0.4412 - precision: 0.9891 - root_mean_squared_error: 0.5000 - val_loss: 0.2409 - val_accuracy: 0.9889 - val_auc: 0.4997 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4908\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2409 - accuracy: 0.9889 - auc: 0.5013 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.4908 - val_loss: 0.2315 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4812\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2316 - accuracy: 0.9889 - auc: 0.4950 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.4813 - val_loss: 0.2198 - val_accuracy: 0.9889 - val_auc: 0.4949 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4689\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2201 - accuracy: 0.9889 - auc: 0.4977 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.4692 - val_loss: 0.2041 - val_accuracy: 0.9889 - val_auc: 0.5000 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4518\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.2046 - accuracy: 0.9889 - auc: 0.5002 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.4523 - val_loss: 0.1824 - val_accuracy: 0.9889 - val_auc: 0.4975 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4271\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1835 - accuracy: 0.9888 - auc: 0.5023 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.4283 - val_loss: 0.1531 - val_accuracy: 0.9889 - val_auc: 0.4885 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.3913\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1556 - accuracy: 0.9888 - auc: 0.4920 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.3945 - val_loss: 0.1191 - val_accuracy: 0.9889 - val_auc: 0.4805 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.3451\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.1238 - accuracy: 0.9887 - auc: 0.5019 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.3518 - val_loss: 0.0892 - val_accuracy: 0.9889 - val_auc: 0.4864 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2987\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0963 - accuracy: 0.9886 - auc: 0.5034 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.3103 - val_loss: 0.0692 - val_accuracy: 0.9889 - val_auc: 0.4881 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2631\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0777 - accuracy: 0.9886 - auc: 0.5088 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2788 - val_loss: 0.0570 - val_accuracy: 0.9889 - val_auc: 0.4854 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2387\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0664 - accuracy: 0.9886 - auc: 0.4980 - recall: 0.9995 - precision: 0.9890 - root_mean_squared_error: 0.2577 - val_loss: 0.0492 - val_accuracy: 0.9889 - val_auc: 0.4871 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2218\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0588 - accuracy: 0.9886 - auc: 0.5062 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2425 - val_loss: 0.0441 - val_accuracy: 0.9889 - val_auc: 0.5014 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2101\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0539 - accuracy: 0.9885 - auc: 0.4910 - recall: 0.9995 - precision: 0.9890 - root_mean_squared_error: 0.2321 - val_loss: 0.0406 - val_accuracy: 0.9889 - val_auc: 0.5014 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2014\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0503 - accuracy: 0.9886 - auc: 0.4906 - recall: 0.9995 - precision: 0.9890 - root_mean_squared_error: 0.2243 - val_loss: 0.0378 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1943\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0475 - accuracy: 0.9886 - auc: 0.5027 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2179 - val_loss: 0.0355 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1883\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0450 - accuracy: 0.9886 - auc: 0.4949 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2121 - val_loss: 0.0335 - val_accuracy: 0.9889 - val_auc: 0.4999 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1831\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0431 - accuracy: 0.9887 - auc: 0.4974 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2077 - val_loss: 0.0318 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1784\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0412 - accuracy: 0.9887 - auc: 0.4952 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2031 - val_loss: 0.0303 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1741\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0396 - accuracy: 0.9888 - auc: 0.4983 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1989 - val_loss: 0.0290 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1702\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0379 - accuracy: 0.9888 - auc: 0.5082 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1948 - val_loss: 0.0278 - val_accuracy: 0.9889 - val_auc: 0.5012 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1667\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0368 - accuracy: 0.9888 - auc: 0.4923 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1919 - val_loss: 0.0267 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1634\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0355 - accuracy: 0.9888 - auc: 0.5046 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1883 - val_loss: 0.0257 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1604\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0345 - accuracy: 0.9888 - auc: 0.4844 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1856 - val_loss: 0.0248 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1576\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0333 - accuracy: 0.9889 - auc: 0.4982 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1825 - val_loss: 0.0240 - val_accuracy: 0.9889 - val_auc: 0.5014 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1550\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0324 - accuracy: 0.9889 - auc: 0.5100 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1800 - val_loss: 0.0233 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1527\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0316 - accuracy: 0.9888 - auc: 0.5030 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1779 - val_loss: 0.0226 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1504\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0308 - accuracy: 0.9889 - auc: 0.4992 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1754 - val_loss: 0.0220 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1484\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0300 - accuracy: 0.9889 - auc: 0.4924 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1732 - val_loss: 0.0214 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1465\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0293 - accuracy: 0.9889 - auc: 0.5069 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1711 - val_loss: 0.0209 - val_accuracy: 0.9889 - val_auc: 0.4344 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1446\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0286 - accuracy: 0.9889 - auc: 0.5040 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1692 - val_loss: 0.0204 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1429\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0279 - accuracy: 0.9889 - auc: 0.5116 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1672 - val_loss: 0.0200 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1413\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0275 - accuracy: 0.9889 - auc: 0.4986 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1659 - val_loss: 0.0195 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1398\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0270 - accuracy: 0.9889 - auc: 0.4861 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1645 - val_loss: 0.0191 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1383\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0264 - accuracy: 0.9889 - auc: 0.4983 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1625 - val_loss: 0.0188 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1370\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0259 - accuracy: 0.9889 - auc: 0.5075 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1610 - val_loss: 0.0184 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1356\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0254 - accuracy: 0.9889 - auc: 0.4992 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1594 - val_loss: 0.0181 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1344\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0249 - accuracy: 0.9889 - auc: 0.5128 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1577 - val_loss: 0.0177 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1332\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0245 - accuracy: 0.9889 - auc: 0.5056 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1565 - val_loss: 0.0174 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1320\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0242 - accuracy: 0.9890 - auc: 0.4980 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1557 - val_loss: 0.0171 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1309\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0237 - accuracy: 0.9890 - auc: 0.4945 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1541 - val_loss: 0.0169 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1299\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0233 - accuracy: 0.9890 - auc: 0.5115 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1525 - val_loss: 0.0166 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1289\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0230 - accuracy: 0.9890 - auc: 0.4977 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1516 - val_loss: 0.0164 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1279\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0226 - accuracy: 0.9890 - auc: 0.5069 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1502 - val_loss: 0.0161 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1269\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0223 - accuracy: 0.9890 - auc: 0.5090 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1492 - val_loss: 0.0159 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1260\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0221 - accuracy: 0.9890 - auc: 0.4894 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1485 - val_loss: 0.0157 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1251\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0216 - accuracy: 0.9890 - auc: 0.5089 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1471 - val_loss: 0.0154 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1243\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0213 - accuracy: 0.9890 - auc: 0.5121 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1458 - val_loss: 0.0152 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1235\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0209 - accuracy: 0.9890 - auc: 0.5066 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1447 - val_loss: 0.0151 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1227\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0207 - accuracy: 0.9890 - auc: 0.5033 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1437 - val_loss: 0.0149 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1219\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 5s 5s/step - loss: 0.0204 - accuracy: 0.9890 - auc: 0.5054 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1429 - val_loss: 0.0147 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0263d35668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U6pSCXepu3v",
        "outputId": "61fdae64-809c-440c-bd63-55ba01482fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "print(model1.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 233634, 20)        2880      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 233634, 20)        0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 233634, 10)        1240      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 233634, 1)         11        \n",
            "=================================================================\n",
            "Total params: 5,811\n",
            "Trainable params: 5,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNquehlPUw-G",
        "outputId": "7ea875ed-9720-414f-cbfb-9c3eafc57948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer1 = ModelCheckpoint(filepath='/content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5', verbose=1, save_best_only=True)\n",
        "hist1 = model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[checkpointer1], verbose = 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01452, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0201 - accuracy: 0.9890 - auc: 0.5070 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1417 - val_loss: 0.0145 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1205\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01452 to 0.01436, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0199 - accuracy: 0.9890 - auc: 0.5077 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1409 - val_loss: 0.0144 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1198\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01436 to 0.01420, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0197 - accuracy: 0.9890 - auc: 0.4915 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1403 - val_loss: 0.0142 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1192\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01420 to 0.01405, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0194 - accuracy: 0.9890 - auc: 0.4965 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1392 - val_loss: 0.0141 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1186\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01405 to 0.01391, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0192 - accuracy: 0.9890 - auc: 0.4870 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1386 - val_loss: 0.0139 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1179\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01391 to 0.01378, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0189 - accuracy: 0.9890 - auc: 0.5012 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1375 - val_loss: 0.0138 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1174\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01378 to 0.01365, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0186 - accuracy: 0.9890 - auc: 0.4984 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1363 - val_loss: 0.0136 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1168\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01365 to 0.01353, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0185 - accuracy: 0.9890 - auc: 0.4998 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1359 - val_loss: 0.0135 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1163\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01353 to 0.01341, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0183 - accuracy: 0.9890 - auc: 0.5051 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1351 - val_loss: 0.0134 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1158\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01341 to 0.01330, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0180 - accuracy: 0.9890 - auc: 0.5032 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1342 - val_loss: 0.0133 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1153\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01330 to 0.01319, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0178 - accuracy: 0.9890 - auc: 0.4993 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1335 - val_loss: 0.0132 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1148\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01319 to 0.01309, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0177 - accuracy: 0.9890 - auc: 0.4938 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1329 - val_loss: 0.0131 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1144\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01309 to 0.01299, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0174 - accuracy: 0.9890 - auc: 0.4975 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1318 - val_loss: 0.0130 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1140\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01299 to 0.01290, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0172 - accuracy: 0.9890 - auc: 0.5013 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1312 - val_loss: 0.0129 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1136\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01290 to 0.01281, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0171 - accuracy: 0.9890 - auc: 0.4924 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1309 - val_loss: 0.0128 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1132\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01281 to 0.01273, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0168 - accuracy: 0.9890 - auc: 0.5195 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1296 - val_loss: 0.0127 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1128\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01273 to 0.01265, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0168 - accuracy: 0.9890 - auc: 0.4985 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1295 - val_loss: 0.0126 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1125\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01265 to 0.01257, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0166 - accuracy: 0.9890 - auc: 0.4976 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1289 - val_loss: 0.0126 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1121\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01257 to 0.01250, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0165 - accuracy: 0.9890 - auc: 0.4868 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1284 - val_loss: 0.0125 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1118\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01250 to 0.01243, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0164 - accuracy: 0.9890 - auc: 0.4962 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1279 - val_loss: 0.0124 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1115\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.01243 to 0.01236, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0162 - accuracy: 0.9890 - auc: 0.4959 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1271 - val_loss: 0.0124 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1112\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01236 to 0.01230, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0160 - accuracy: 0.9890 - auc: 0.4998 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1266 - val_loss: 0.0123 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1109\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01230 to 0.01224, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0159 - accuracy: 0.9890 - auc: 0.5028 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1262 - val_loss: 0.0122 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1106\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01224 to 0.01218, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0158 - accuracy: 0.9890 - auc: 0.5015 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1256 - val_loss: 0.0122 - val_accuracy: 0.9889 - val_auc: 0.5024 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1104\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01218 to 0.01213, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0157 - accuracy: 0.9890 - auc: 0.4983 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1252 - val_loss: 0.0121 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1101\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01213 to 0.01208, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0156 - accuracy: 0.9890 - auc: 0.4953 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1247 - val_loss: 0.0121 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1099\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01208 to 0.01203, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0154 - accuracy: 0.9890 - auc: 0.4952 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1242 - val_loss: 0.0120 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1097\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01203 to 0.01198, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0153 - accuracy: 0.9890 - auc: 0.4831 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1238 - val_loss: 0.0120 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1095\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01198 to 0.01194, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0151 - accuracy: 0.9890 - auc: 0.5003 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1231 - val_loss: 0.0119 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1092\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.01194 to 0.01189, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0151 - accuracy: 0.9890 - auc: 0.4971 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1230 - val_loss: 0.0119 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1090\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.01189 to 0.01185, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0150 - accuracy: 0.9890 - auc: 0.4992 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1224 - val_loss: 0.0119 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1089\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.01185 to 0.01181, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0149 - accuracy: 0.9890 - auc: 0.4935 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1222 - val_loss: 0.0118 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1087\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.01181 to 0.01177, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0148 - accuracy: 0.9890 - auc: 0.5010 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1217 - val_loss: 0.0118 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1085\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.01177 to 0.01174, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0147 - accuracy: 0.9890 - auc: 0.4940 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1213 - val_loss: 0.0117 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1083\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.01174 to 0.01170, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0147 - accuracy: 0.9890 - auc: 0.4897 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1211 - val_loss: 0.0117 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1082\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.01170 to 0.01167, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0145 - accuracy: 0.9890 - auc: 0.5085 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1206 - val_loss: 0.0117 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1080\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.01167 to 0.01164, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0144 - accuracy: 0.9890 - auc: 0.5060 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1202 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1079\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.01164 to 0.01161, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0143 - accuracy: 0.9890 - auc: 0.5005 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1198 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1077\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.01161 to 0.01158, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0143 - accuracy: 0.9890 - auc: 0.5031 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1195 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1076\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.01158 to 0.01155, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0142 - accuracy: 0.9890 - auc: 0.4992 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1191 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1075\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.01155 to 0.01153, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0141 - accuracy: 0.9890 - auc: 0.5099 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1189 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1074\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.01153 to 0.01150, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0141 - accuracy: 0.9890 - auc: 0.4911 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1187 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1072\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.01150 to 0.01148, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0140 - accuracy: 0.9890 - auc: 0.5008 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1182 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1071\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.01148 to 0.01146, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0140 - accuracy: 0.9890 - auc: 0.4928 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1183 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1070\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.01146 to 0.01143, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0139 - accuracy: 0.9890 - auc: 0.5025 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1177 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1069\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.01143 to 0.01141, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0138 - accuracy: 0.9890 - auc: 0.5049 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1173 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1068\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.01141 to 0.01139, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0137 - accuracy: 0.9890 - auc: 0.5120 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1172 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1067\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.01139 to 0.01137, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0137 - accuracy: 0.9890 - auc: 0.4924 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1172 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1067\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.01137 to 0.01136, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0136 - accuracy: 0.9890 - auc: 0.4917 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1168 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1066\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.01136 to 0.01134, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0135 - accuracy: 0.9890 - auc: 0.5111 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1164 - val_loss: 0.0113 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzR0XD3hVAPT",
        "outputId": "9cb9a48a-3e99-4752-ef5f-e846faac7a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "scores1 = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print('UDPlag')\n",
        "print(\"Loss: %.2f%%\" % (scores1[0]*100))\n",
        "print(\"Acurácia: %.2f%%\" % (scores1[1]*100))\n",
        "print(\"AUC: %.2f%%\" % (scores1[2]*100))\n",
        "print(\"Recall: %.2f%%\" % (scores1[3]*100))\n",
        "print(\"Precision: %.2f%%\" % (scores1[4]*100))\n",
        "print(\"RootMeanSquaredError: %.2f%%\" % (scores1[5]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UDPlag\n",
            "Loss: 1.13%\n",
            "Acurácia: 98.89%\n",
            "AUC: 50.03%\n",
            "Recall: 100.00%\n",
            "Precision: 98.89%\n",
            "RootMeanSquaredError: 10.65%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju2WJkqIo7di"
      },
      "source": [
        "#### Experimento 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOfAv2_bJowY",
        "outputId": "2dc63c77-85ec-4a3a-8e88-06429c3b7147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "plt.title('UDP-Lag')\n",
        "plt.plot(hist1.history['loss'], label='train')\n",
        "plt.plot(hist1.history['val_loss'], label='test')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}