{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearning_UDPlag.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZRSiDYNQLSr",
        "outputId": "9ca9c6de-3ec6-4a85-d87c-a4ab531f3667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6SYRFm9mr-"
      },
      "source": [
        "### Carregamento arquivo de pré-processamento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NY7uDiB9AG57",
        "outputId": "08edae5b-3517-4c25-8447-0c46f6227949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "%run \"/content/drive/My Drive/pre_processamento_TCC.ipynb\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "CPU times: user 1min 49s, sys: 11.6 s, total: 2min\n",
            "Wall time: 2min 9s\n",
            "Ataque de exploração UDPLag:  Label\n",
            "BENIGN       3705\n",
            "UDP-lag    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "BENIGN        392\n",
            "Syn       1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "BENIGN           1612\n",
            "DrDoS_LDAP    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "BENIGN              1707\n",
            "DrDoS_NetBIOS    4093279\n",
            "dtype: int64\n",
            "Ataque de exploração UDPLag:  Label\n",
            "0      3705\n",
            "1    366461\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração Syn:  Label\n",
            "0        392\n",
            "1    1582289\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração LDAP:  Label\n",
            "0       1612\n",
            "1    2179930\n",
            "dtype: int64 \n",
            "\n",
            "Ataque de exploração NetBIOS:  Label\n",
            "0       1707\n",
            "1    4093279\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXdSSkdaH4WQ"
      },
      "source": [
        "### Bibliotecas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMyNbLHJIBlJ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest \n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dense, Dropout, LSTM\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSWBHgHzHU9w"
      },
      "source": [
        "### Divisão do conjunto em treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N53UJNEuJCQf",
        "outputId": "dcd80e00-ce95-4ac9-9342-4e4d955e2deb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "UDPlag"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Fwd Header Length.1</th>\n",
              "      <th>Fwd Avg Bytes/Bulk</th>\n",
              "      <th>Fwd Avg Packets/Bulk</th>\n",
              "      <th>Fwd Avg Bulk Rate</th>\n",
              "      <th>Bwd Avg Bytes/Bulk</th>\n",
              "      <th>Bwd Avg Packets/Bulk</th>\n",
              "      <th>Bwd Avg Bulk Rate</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>766.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>766000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>574.5</td>\n",
              "      <td>383.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>766</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>778.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>778000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>583.5</td>\n",
              "      <td>389.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>778</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375000000.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>562.5</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>738.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>369000000.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>553.5</td>\n",
              "      <td>369.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>738</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>750.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>750000000.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>562.5</td>\n",
              "      <td>375.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>750</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370599</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370600</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370601</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>2.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370602</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>1.000000e+06</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370604</th>\n",
              "      <td>134</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.985075e+04</td>\n",
              "      <td>44.666667</td>\n",
              "      <td>75.632885</td>\n",
              "      <td>132.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>40</td>\n",
              "      <td>1.492537e+04</td>\n",
              "      <td>14925.373134</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>5840</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>333763 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Flow Duration   Total Fwd Packets  ...   Idle Min   Label\n",
              "0                    1                   2  ...        0.0       1\n",
              "1                    1                   2  ...        0.0       1\n",
              "2                    2                   2  ...        0.0       1\n",
              "3                    2                   2  ...        0.0       1\n",
              "4                    1                   2  ...        0.0       1\n",
              "...                ...                 ...  ...        ...     ...\n",
              "370599               1                   2  ...        0.0       1\n",
              "370600               1                   2  ...        0.0       1\n",
              "370601               1                   2  ...        0.0       1\n",
              "370602               2                   2  ...        0.0       1\n",
              "370604             134                   2  ...        0.0       1\n",
              "\n",
              "[333763 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ldV1PAMGk6h"
      },
      "source": [
        "Preparação dos dados treino e teste\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1JQDSHBGMm1"
      },
      "source": [
        "X = UDPlag.iloc[:, 0:77]\n",
        "y = UDPlag.iloc[:,- 1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY1hcfSsGQEj"
      },
      "source": [
        "70% para treino, 30% para teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmiwVL7bDRaL"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=0.3,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWSoJT6XFgyZ",
        "outputId": "2a1cff0b-2048-4d1d-b2ae-7f0fa6857983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Formato dos dados de entrada\n",
        "print('Formato dos dados de entrada:', x_train.shape)\n",
        "\n",
        "# Tamanho dos conjuntos\n",
        "print('Amostras de treino: ', x_train.shape[0])\n",
        "print('Amostras de teste: ', x_test.shape[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Formato dos dados de entrada: (233634, 77)\n",
            "Amostras de treino:  233634\n",
            "Amostras de teste:  100129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGh7k5c6Hd2e"
      },
      "source": [
        "### Seleção dos Parâmetro\n",
        "Seleção dos 15 melhores parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UNx2xDtDg6K",
        "outputId": "d223098e-4da2-4d81-cfb1-eaa5ca6cb210",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "best_features = SelectKBest(score_func=f_classif, k=15)\n",
        "fit = best_features.fit(x_train,y_train)\n",
        "df_scores = pd.DataFrame(fit.scores_)\n",
        "df_columns = pd.DataFrame(x_train.columns)\n",
        "# concatenar quadros de dados\n",
        "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
        "feature_scores.columns = ['Feature_Name','Score']  # colunas de saída de nome\n",
        "print(feature_scores.nlargest(15,'Score'))  # imprima 15 melhores parâmetros"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                Feature_Name          Score\n",
            "47            URG Flag Count  110569.603753\n",
            "48            CWE Flag Count   50096.792295\n",
            "11    Bwd Packet Length Mean   33925.850698\n",
            "53      Avg Bwd Segment Size   33925.850698\n",
            "9      Bwd Packet Length Max   32796.113276\n",
            "29             Fwd PSH Flags   29159.350848\n",
            "44            RST Flag Count   29159.350848\n",
            "50             Down/Up Ratio   27909.522885\n",
            "40         Packet Length Std   26857.407173\n",
            "66   Init_Win_bytes_backward   26760.422368\n",
            "12     Bwd Packet Length Std   23563.411565\n",
            "34         Bwd Header Length   23133.283185\n",
            "8      Fwd Packet Length Std   18141.562396\n",
            "10     Bwd Packet Length Min   17837.128675\n",
            "41    Packet Length Variance   15762.971567\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [30 31 32 42 45 49 55 56 57 58 59 60] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8TRpDjFn-OC",
        "outputId": "95403047-1cc3-4e8f-d635-17d0ec5d7d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "feature = feature_scores.nlargest(15,'Score')\n",
        "feature"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature_Name</th>\n",
              "      <th>Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>URG Flag Count</td>\n",
              "      <td>110569.603753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>CWE Flag Count</td>\n",
              "      <td>50096.792295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Bwd Packet Length Mean</td>\n",
              "      <td>33925.850698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Avg Bwd Segment Size</td>\n",
              "      <td>33925.850698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Bwd Packet Length Max</td>\n",
              "      <td>32796.113276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Fwd PSH Flags</td>\n",
              "      <td>29159.350848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>RST Flag Count</td>\n",
              "      <td>29159.350848</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Down/Up Ratio</td>\n",
              "      <td>27909.522885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Packet Length Std</td>\n",
              "      <td>26857.407173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Init_Win_bytes_backward</td>\n",
              "      <td>26760.422368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Bwd Packet Length Std</td>\n",
              "      <td>23563.411565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Bwd Header Length</td>\n",
              "      <td>23133.283185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Fwd Packet Length Std</td>\n",
              "      <td>18141.562396</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Bwd Packet Length Min</td>\n",
              "      <td>17837.128675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Packet Length Variance</td>\n",
              "      <td>15762.971567</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                Feature_Name          Score\n",
              "47            URG Flag Count  110569.603753\n",
              "48            CWE Flag Count   50096.792295\n",
              "11    Bwd Packet Length Mean   33925.850698\n",
              "53      Avg Bwd Segment Size   33925.850698\n",
              "9      Bwd Packet Length Max   32796.113276\n",
              "29             Fwd PSH Flags   29159.350848\n",
              "44            RST Flag Count   29159.350848\n",
              "50             Down/Up Ratio   27909.522885\n",
              "40         Packet Length Std   26857.407173\n",
              "66   Init_Win_bytes_backward   26760.422368\n",
              "12     Bwd Packet Length Std   23563.411565\n",
              "34         Bwd Header Length   23133.283185\n",
              "8      Fwd Packet Length Std   18141.562396\n",
              "10     Bwd Packet Length Min   17837.128675\n",
              "41    Packet Length Variance   15762.971567"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9x3SW1DzTu"
      },
      "source": [
        "Exlusão dos parâmetros que não seram usados no modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfzZxvkcEGm0",
        "outputId": "44bb085f-0eb2-4f1c-e833-8ca4fd304bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "netbios.columns"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([' Flow Duration', ' Total Fwd Packets', ' Total Backward Packets',\n",
              "       'Total Length of Fwd Packets', ' Total Length of Bwd Packets',\n",
              "       ' Fwd Packet Length Max', ' Fwd Packet Length Min',\n",
              "       ' Fwd Packet Length Mean', ' Fwd Packet Length Std',\n",
              "       'Bwd Packet Length Max', ' Bwd Packet Length Min',\n",
              "       ' Bwd Packet Length Mean', ' Bwd Packet Length Std', 'Flow Bytes/s',\n",
              "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
              "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
              "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
              "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags',\n",
              "       ' Bwd PSH Flags', ' Fwd URG Flags', ' Bwd URG Flags',\n",
              "       ' Fwd Header Length', ' Bwd Header Length', 'Fwd Packets/s',\n",
              "       ' Bwd Packets/s', ' Min Packet Length', ' Max Packet Length',\n",
              "       ' Packet Length Mean', ' Packet Length Std', ' Packet Length Variance',\n",
              "       'FIN Flag Count', ' SYN Flag Count', ' RST Flag Count',\n",
              "       ' PSH Flag Count', ' ACK Flag Count', ' URG Flag Count',\n",
              "       ' CWE Flag Count', ' ECE Flag Count', ' Down/Up Ratio',\n",
              "       ' Average Packet Size', ' Avg Fwd Segment Size',\n",
              "       ' Avg Bwd Segment Size', ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk',\n",
              "       ' Fwd Avg Packets/Bulk', ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk',\n",
              "       ' Bwd Avg Packets/Bulk', 'Bwd Avg Bulk Rate', 'Subflow Fwd Packets',\n",
              "       ' Subflow Fwd Bytes', ' Subflow Bwd Packets', ' Subflow Bwd Bytes',\n",
              "       'Init_Win_bytes_forward', ' Init_Win_bytes_backward',\n",
              "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean',\n",
              "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
              "       ' Idle Max', ' Idle Min', ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jLjyHynEVY-"
      },
      "source": [
        "x_train = x_train.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)\n",
        "\n",
        "x_test = x_test.drop(columns=[' Flow Duration', ' Total Fwd Packets',\n",
        "       ' Total Backward Packets','Total Length of Fwd Packets', \n",
        "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max', \n",
        "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean', 'Flow Bytes/s',\n",
        "       ' Flow Packets/s', ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max',\n",
        "       ' Flow IAT Min', 'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std',\n",
        "       ' Fwd IAT Max', ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean',\n",
        "       ' Bwd IAT Std', ' Bwd IAT Max', ' Bwd IAT Min', ' Bwd PSH Flags', \n",
        "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length', \n",
        "       'Fwd Packets/s', ' Bwd Packets/s', ' Min Packet Length', \n",
        "       ' Max Packet Length', ' Packet Length Mean', 'FIN Flag Count', \n",
        "       ' SYN Flag Count', ' PSH Flag Count', ' ACK Flag Count', \n",
        "       ' ECE Flag Count', ' Average Packet Size', ' Avg Fwd Segment Size',\n",
        "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk', \n",
        "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk', \n",
        "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes', \n",
        "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
        "       ' act_data_pkt_fwd', ' min_seg_size_forward', 'Active Mean', \n",
        "       ' Active Std', ' Active Max', ' Active Min', 'Idle Mean', ' Idle Std',\n",
        "       ' Idle Max', ' Idle Min'], axis= 1)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARPy2izzJFxW"
      },
      "source": [
        "### Normalização dos dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oaGTiRsYjln"
      },
      "source": [
        "normalizador = MinMaxScaler()\n",
        "x_train= normalizador.fit_transform(x_train)\n",
        "x_test = normalizador.fit_transform(x_test)\n",
        "y_train = y_train.values.reshape(-1,1)\n",
        "y_test = y_test.values.reshape(-1,1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCe_uiUlpIzN"
      },
      "source": [
        "### Formatação do tensor em 3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmYg8yFCIUBQ"
      },
      "source": [
        "x_train= x_train.reshape(-1, 233634, 15)\n",
        "y_train= y_train.reshape(-1, 233634, 1)\n",
        "x_test = x_test.reshape(-1, 100129, 15)\n",
        "y_test = y_test.reshape(-1, 100129, 1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-eJBhktpWyk",
        "outputId": "fe0395e1-308d-4b53-c24c-49244b9a2488",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 233634, 15)\n",
            "(1, 233634, 1)\n",
            "(1, 100129, 15)\n",
            "(1, 100129, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj4z-gkHpcYD"
      },
      "source": [
        "### Rede Neural Recorrente (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAxNKXpNuvT"
      },
      "source": [
        "#### Experimento 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_PrK6L9plVz",
        "outputId": "120c1cc3-6d8d-4da3-ab15-7a4fa16b81d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(units = 20, return_sequences = True, input_shape=(233634, 15)))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "model1.add(LSTM(units = 10, return_sequences = True))\n",
        "model1.add(Dropout(0.3))\n",
        "\n",
        "# Camada Final\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile model\n",
        "model1.compile(optimizer = 'RMSprop', loss = 'mean_squared_error',\n",
        "                  metrics=['accuracy', 'AUC', 'Recall', 'Precision', 'RootMeanSquaredError'])\n",
        "# Fit the model\n",
        "model1.fit(x_train,y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - ETA: 0s - loss: 0.2500 - accuracy: 0.6017 - auc: 0.4994 - recall: 0.6031 - precision: 0.9905 - root_mean_squared_error: 0.5000WARNING:tensorflow:Model was constructed with shape (None, 233634, 15) for input Tensor(\"lstm_input:0\", shape=(None, 233634, 15), dtype=float32), but it was called on an input with incompatible shape (None, 100129, 15).\n",
            "1/1 [==============================] - 6s 6s/step - loss: 0.2500 - accuracy: 0.6017 - auc: 0.4994 - recall: 0.6031 - precision: 0.9905 - root_mean_squared_error: 0.5000 - val_loss: 0.2438 - val_accuracy: 0.9889 - val_auc: 0.5002 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4938\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2439 - accuracy: 0.9885 - auc: 0.5018 - recall: 0.9994 - precision: 0.9890 - root_mean_squared_error: 0.4939 - val_loss: 0.2383 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4881\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2383 - accuracy: 0.9882 - auc: 0.4988 - recall: 0.9992 - precision: 0.9890 - root_mean_squared_error: 0.4882 - val_loss: 0.2318 - val_accuracy: 0.9889 - val_auc: 0.4977 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4815\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2319 - accuracy: 0.9872 - auc: 0.5012 - recall: 0.9982 - precision: 0.9890 - root_mean_squared_error: 0.4816 - val_loss: 0.2236 - val_accuracy: 0.9889 - val_auc: 0.4918 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4729\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2239 - accuracy: 0.9858 - auc: 0.5025 - recall: 0.9967 - precision: 0.9890 - root_mean_squared_error: 0.4732 - val_loss: 0.2124 - val_accuracy: 0.9889 - val_auc: 0.5000 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4609\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.2129 - accuracy: 0.9841 - auc: 0.5003 - recall: 0.9950 - precision: 0.9890 - root_mean_squared_error: 0.4615 - val_loss: 0.1962 - val_accuracy: 0.9889 - val_auc: 0.4905 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4430\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1974 - accuracy: 0.9825 - auc: 0.4947 - recall: 0.9933 - precision: 0.9890 - root_mean_squared_error: 0.4443 - val_loss: 0.1726 - val_accuracy: 0.9889 - val_auc: 0.4953 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.4154\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1752 - accuracy: 0.9807 - auc: 0.4984 - recall: 0.9915 - precision: 0.9890 - root_mean_squared_error: 0.4185 - val_loss: 0.1430 - val_accuracy: 0.9889 - val_auc: 0.4981 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.3781\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1484 - accuracy: 0.9813 - auc: 0.5079 - recall: 0.9921 - precision: 0.9890 - root_mean_squared_error: 0.3852 - val_loss: 0.1182 - val_accuracy: 0.9889 - val_auc: 0.4964 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.3438\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1260 - accuracy: 0.9825 - auc: 0.4873 - recall: 0.9933 - precision: 0.9890 - root_mean_squared_error: 0.3550 - val_loss: 0.1011 - val_accuracy: 0.9889 - val_auc: 0.4961 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.3180\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.1104 - accuracy: 0.9851 - auc: 0.4977 - recall: 0.9960 - precision: 0.9890 - root_mean_squared_error: 0.3322 - val_loss: 0.0878 - val_accuracy: 0.9889 - val_auc: 0.5017 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2964\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0984 - accuracy: 0.9863 - auc: 0.4873 - recall: 0.9972 - precision: 0.9890 - root_mean_squared_error: 0.3137 - val_loss: 0.0780 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2793\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0892 - accuracy: 0.9875 - auc: 0.5011 - recall: 0.9985 - precision: 0.9890 - root_mean_squared_error: 0.2987 - val_loss: 0.0707 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2660\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0823 - accuracy: 0.9880 - auc: 0.5068 - recall: 0.9989 - precision: 0.9890 - root_mean_squared_error: 0.2870 - val_loss: 0.0649 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2547\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0768 - accuracy: 0.9882 - auc: 0.5035 - recall: 0.9992 - precision: 0.9890 - root_mean_squared_error: 0.2771 - val_loss: 0.0600 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2449\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0720 - accuracy: 0.9885 - auc: 0.4940 - recall: 0.9994 - precision: 0.9890 - root_mean_squared_error: 0.2684 - val_loss: 0.0557 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2360\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0680 - accuracy: 0.9885 - auc: 0.5082 - recall: 0.9995 - precision: 0.9890 - root_mean_squared_error: 0.2608 - val_loss: 0.0519 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2277\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0641 - accuracy: 0.9886 - auc: 0.5042 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2531 - val_loss: 0.0484 - val_accuracy: 0.9889 - val_auc: 0.5014 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2201\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0607 - accuracy: 0.9886 - auc: 0.4952 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2463 - val_loss: 0.0453 - val_accuracy: 0.9889 - val_auc: 0.5001 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2128\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0574 - accuracy: 0.9886 - auc: 0.4975 - recall: 0.9996 - precision: 0.9890 - root_mean_squared_error: 0.2396 - val_loss: 0.0424 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.2059\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0545 - accuracy: 0.9887 - auc: 0.4992 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2335 - val_loss: 0.0398 - val_accuracy: 0.9889 - val_auc: 0.5013 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1994\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0518 - accuracy: 0.9886 - auc: 0.5055 - recall: 0.9995 - precision: 0.9890 - root_mean_squared_error: 0.2276 - val_loss: 0.0373 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1932\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0492 - accuracy: 0.9887 - auc: 0.5031 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2219 - val_loss: 0.0351 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1874\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0469 - accuracy: 0.9887 - auc: 0.5040 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2166 - val_loss: 0.0331 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1819\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0448 - accuracy: 0.9887 - auc: 0.4957 - recall: 0.9997 - precision: 0.9890 - root_mean_squared_error: 0.2116 - val_loss: 0.0312 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1767\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0427 - accuracy: 0.9888 - auc: 0.4949 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.2066 - val_loss: 0.0296 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1719\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0408 - accuracy: 0.9888 - auc: 0.4960 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.2020 - val_loss: 0.0281 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1675\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0391 - accuracy: 0.9888 - auc: 0.5008 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1977 - val_loss: 0.0267 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1635\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0375 - accuracy: 0.9888 - auc: 0.4976 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1936 - val_loss: 0.0255 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1598\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0360 - accuracy: 0.9888 - auc: 0.5003 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1896 - val_loss: 0.0245 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1565\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0346 - accuracy: 0.9888 - auc: 0.5063 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1861 - val_loss: 0.0235 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1534\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0335 - accuracy: 0.9889 - auc: 0.4965 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1830 - val_loss: 0.0227 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1507\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0325 - accuracy: 0.9888 - auc: 0.4957 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1801 - val_loss: 0.0219 - val_accuracy: 0.9889 - val_auc: 0.3602 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1481\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0313 - accuracy: 0.9889 - auc: 0.5049 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1768 - val_loss: 0.0213 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1458\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0303 - accuracy: 0.9888 - auc: 0.5071 - recall: 0.9998 - precision: 0.9890 - root_mean_squared_error: 0.1742 - val_loss: 0.0206 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1437\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0295 - accuracy: 0.9889 - auc: 0.5008 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1717 - val_loss: 0.0201 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1417\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0288 - accuracy: 0.9889 - auc: 0.5023 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1696 - val_loss: 0.0196 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1399\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0281 - accuracy: 0.9889 - auc: 0.4854 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1676 - val_loss: 0.0191 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1382\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0272 - accuracy: 0.9889 - auc: 0.4994 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1650 - val_loss: 0.0187 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1367\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0266 - accuracy: 0.9890 - auc: 0.4995 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1632 - val_loss: 0.0183 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1352\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0261 - accuracy: 0.9890 - auc: 0.5025 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1617 - val_loss: 0.0179 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1338\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0255 - accuracy: 0.9889 - auc: 0.5039 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1597 - val_loss: 0.0176 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1326\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0250 - accuracy: 0.9890 - auc: 0.5022 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1583 - val_loss: 0.0173 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1314\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0246 - accuracy: 0.9889 - auc: 0.4937 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1568 - val_loss: 0.0170 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1303\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0241 - accuracy: 0.9890 - auc: 0.4928 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1552 - val_loss: 0.0167 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1292\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0236 - accuracy: 0.9890 - auc: 0.4993 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1537 - val_loss: 0.0164 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1282\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0233 - accuracy: 0.9890 - auc: 0.4851 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1526 - val_loss: 0.0162 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1273\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0227 - accuracy: 0.9890 - auc: 0.5027 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1506 - val_loss: 0.0160 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1264\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0225 - accuracy: 0.9889 - auc: 0.5037 - recall: 0.9999 - precision: 0.9890 - root_mean_squared_error: 0.1499 - val_loss: 0.0158 - val_accuracy: 0.9889 - val_auc: 0.5010 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1255\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 4s 4s/step - loss: 0.0221 - accuracy: 0.9890 - auc: 0.5067 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1486 - val_loss: 0.0156 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1247\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f918aab2668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U6pSCXepu3v",
        "outputId": "c38884c5-48ca-4108-c08f-4f36f80bda4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "print(model1.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 233634, 20)        2880      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 233634, 20)        0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 233634, 10)        1240      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 233634, 10)        840       \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 233634, 10)        0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 233634, 1)         11        \n",
            "=================================================================\n",
            "Total params: 5,811\n",
            "Trainable params: 5,811\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNquehlPUw-G",
        "outputId": "656c1520-da51-4fd5-e34f-a31aadc56d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "checkpointer1 = ModelCheckpoint(filepath='/content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5', verbose=1, save_best_only=True)\n",
        "hist1 = model1.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=50, batch_size=32, callbacks=[checkpointer1], verbose = 2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01536, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 4s - loss: 0.0218 - accuracy: 0.9890 - auc: 0.5026 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1475 - val_loss: 0.0154 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1240\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01536 to 0.01518, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0214 - accuracy: 0.9890 - auc: 0.4944 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1463 - val_loss: 0.0152 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1232\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01518 to 0.01501, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0211 - accuracy: 0.9890 - auc: 0.5071 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1453 - val_loss: 0.0150 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1225\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.01501 to 0.01485, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0209 - accuracy: 0.9890 - auc: 0.4988 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1446 - val_loss: 0.0149 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1219\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01485 to 0.01470, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0206 - accuracy: 0.9890 - auc: 0.5064 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1435 - val_loss: 0.0147 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1212\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01470 to 0.01455, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0203 - accuracy: 0.9890 - auc: 0.5066 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1425 - val_loss: 0.0146 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1206\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01455 to 0.01441, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0200 - accuracy: 0.9890 - auc: 0.5036 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1414 - val_loss: 0.0144 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1200\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.01441 to 0.01428, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0199 - accuracy: 0.9890 - auc: 0.4990 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1410 - val_loss: 0.0143 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1195\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.01428 to 0.01415, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0196 - accuracy: 0.9890 - auc: 0.5021 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1399 - val_loss: 0.0141 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1189\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.01415 to 0.01403, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0194 - accuracy: 0.9890 - auc: 0.4980 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1391 - val_loss: 0.0140 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1184\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.01403 to 0.01391, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0191 - accuracy: 0.9890 - auc: 0.4969 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1383 - val_loss: 0.0139 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1179\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.01391 to 0.01380, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0189 - accuracy: 0.9890 - auc: 0.4974 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1374 - val_loss: 0.0138 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1175\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01380 to 0.01369, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0188 - accuracy: 0.9890 - auc: 0.4986 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1371 - val_loss: 0.0137 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1170\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01369 to 0.01358, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0185 - accuracy: 0.9890 - auc: 0.5080 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1360 - val_loss: 0.0136 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1165\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01358 to 0.01348, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0183 - accuracy: 0.9890 - auc: 0.5030 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1353 - val_loss: 0.0135 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1161\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.01348 to 0.01339, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0181 - accuracy: 0.9890 - auc: 0.5057 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1347 - val_loss: 0.0134 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1157\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01339 to 0.01329, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0180 - accuracy: 0.9890 - auc: 0.4942 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1342 - val_loss: 0.0133 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1153\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.01329 to 0.01320, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0178 - accuracy: 0.9890 - auc: 0.4959 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1335 - val_loss: 0.0132 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1149\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01320 to 0.01311, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0177 - accuracy: 0.9890 - auc: 0.4863 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1330 - val_loss: 0.0131 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1145\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01311 to 0.01303, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0175 - accuracy: 0.9890 - auc: 0.5010 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1323 - val_loss: 0.0130 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1141\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.01303 to 0.01295, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0173 - accuracy: 0.9890 - auc: 0.5067 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1315 - val_loss: 0.0129 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1138\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.01295 to 0.01286, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0171 - accuracy: 0.9890 - auc: 0.5044 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1309 - val_loss: 0.0129 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1134\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.01286 to 0.01279, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0170 - accuracy: 0.9890 - auc: 0.4933 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1305 - val_loss: 0.0128 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1131\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.01279 to 0.01271, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 4s - loss: 0.0169 - accuracy: 0.9890 - auc: 0.4896 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1299 - val_loss: 0.0127 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1127\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01271 to 0.01264, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0166 - accuracy: 0.9890 - auc: 0.5079 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1290 - val_loss: 0.0126 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1124\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01264 to 0.01256, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0166 - accuracy: 0.9890 - auc: 0.4837 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1289 - val_loss: 0.0126 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1121\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.01256 to 0.01249, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0164 - accuracy: 0.9890 - auc: 0.5046 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1281 - val_loss: 0.0125 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1118\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.01249 to 0.01243, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0162 - accuracy: 0.9890 - auc: 0.5100 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1274 - val_loss: 0.0124 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1115\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.01243 to 0.01236, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0161 - accuracy: 0.9890 - auc: 0.4997 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1270 - val_loss: 0.0124 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1112\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.01236 to 0.01229, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0160 - accuracy: 0.9890 - auc: 0.5013 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1263 - val_loss: 0.0123 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1109\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.01229 to 0.01223, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0158 - accuracy: 0.9890 - auc: 0.5013 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1257 - val_loss: 0.0122 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1106\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.01223 to 0.01217, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0156 - accuracy: 0.9890 - auc: 0.5099 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1251 - val_loss: 0.0122 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1103\n",
            "Epoch 33/50\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.01217 to 0.01211, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0155 - accuracy: 0.9890 - auc: 0.5077 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1246 - val_loss: 0.0121 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1101\n",
            "Epoch 34/50\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.01211 to 0.01205, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0154 - accuracy: 0.9890 - auc: 0.5041 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1241 - val_loss: 0.0121 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1098\n",
            "Epoch 35/50\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.01205 to 0.01200, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0153 - accuracy: 0.9890 - auc: 0.5029 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1235 - val_loss: 0.0120 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1095\n",
            "Epoch 36/50\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.01200 to 0.01195, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0151 - accuracy: 0.9890 - auc: 0.5105 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1229 - val_loss: 0.0119 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1093\n",
            "Epoch 37/50\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.01195 to 0.01190, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0150 - accuracy: 0.9890 - auc: 0.5088 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1225 - val_loss: 0.0119 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1091\n",
            "Epoch 38/50\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.01190 to 0.01185, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0149 - accuracy: 0.9890 - auc: 0.5036 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1222 - val_loss: 0.0118 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1088\n",
            "Epoch 39/50\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.01185 to 0.01180, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0148 - accuracy: 0.9890 - auc: 0.4977 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1218 - val_loss: 0.0118 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1086\n",
            "Epoch 40/50\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.01180 to 0.01175, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0147 - accuracy: 0.9890 - auc: 0.4896 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1211 - val_loss: 0.0118 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1084\n",
            "Epoch 41/50\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.01175 to 0.01171, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0146 - accuracy: 0.9890 - auc: 0.4921 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1208 - val_loss: 0.0117 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1082\n",
            "Epoch 42/50\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.01171 to 0.01167, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0144 - accuracy: 0.9890 - auc: 0.5029 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1201 - val_loss: 0.0117 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1080\n",
            "Epoch 43/50\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.01167 to 0.01163, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0143 - accuracy: 0.9890 - auc: 0.4999 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1198 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1079\n",
            "Epoch 44/50\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.01163 to 0.01160, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0142 - accuracy: 0.9890 - auc: 0.5049 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1193 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5006 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1077\n",
            "Epoch 45/50\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.01160 to 0.01156, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0141 - accuracy: 0.9890 - auc: 0.5089 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1189 - val_loss: 0.0116 - val_accuracy: 0.9889 - val_auc: 0.5007 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1075\n",
            "Epoch 46/50\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.01156 to 0.01153, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0141 - accuracy: 0.9890 - auc: 0.4967 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1187 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1074\n",
            "Epoch 47/50\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.01153 to 0.01150, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0139 - accuracy: 0.9890 - auc: 0.5015 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1180 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1072\n",
            "Epoch 48/50\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.01150 to 0.01147, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0139 - accuracy: 0.9890 - auc: 0.5080 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1178 - val_loss: 0.0115 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1071\n",
            "Epoch 49/50\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.01147 to 0.01144, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0138 - accuracy: 0.9890 - auc: 0.4927 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1175 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1070\n",
            "Epoch 50/50\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.01144 to 0.01142, saving model to /content/drive/My Drive/Colab Notebooks/model.UDPlag.hdf5\n",
            "1/1 - 5s - loss: 0.0137 - accuracy: 0.9890 - auc: 0.4911 - recall: 1.0000 - precision: 0.9890 - root_mean_squared_error: 0.1172 - val_loss: 0.0114 - val_accuracy: 0.9889 - val_auc: 0.5003 - val_recall: 1.0000 - val_precision: 0.9889 - val_root_mean_squared_error: 0.1069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzR0XD3hVAPT",
        "outputId": "02defcd7-29d1-410e-8458-98f4b1928024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "scores1 = model1.evaluate(x_test, y_test, verbose=0)\n",
        "print('UDPlag')\n",
        "print(\"Loss: %.2f%%\" % (scores1[0]*100))\n",
        "print(\"Acurácia: %.2f%%\" % (scores1[1]*100))\n",
        "print(\"AUC: %.2f%%\" % (scores1[2]*100))\n",
        "print(\"Recall: %.2f%%\" % (scores1[3]*100))\n",
        "print(\"Precision: %.2f%%\" % (scores1[4]*100))\n",
        "print(\"RootMeanSquaredError: %.2f%%\" % (scores1[5]*100))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UDPlag\n",
            "Loss: 1.14%\n",
            "Acurácia: 98.89%\n",
            "AUC: 50.03%\n",
            "Recall: 100.00%\n",
            "Precision: 98.89%\n",
            "RootMeanSquaredError: 10.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ju2WJkqIo7di"
      },
      "source": [
        "#### Experimento 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOfAv2_bJowY",
        "outputId": "a02e3647-a15c-4f64-c38b-6a6e41e01357",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.title('UDP-Lag')\n",
        "plt.plot(hist1.history['loss'], label='train')\n",
        "plt.plot(hist1.history['val_loss'], label='test')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}